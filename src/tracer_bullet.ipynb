{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01414296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\SummaryBot\\sb_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from ollama import pull, generate\n",
    "import math\n",
    "import glob\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64325e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"gemma3:4b\"\n",
    "pull(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2b4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(input_path: str) -> list[str]:\n",
    "    try:\n",
    "        with pymupdf.open(input_path) as pdf:\n",
    "            text_by_page = [page.get_text() for page in pdf]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"PDF file processing failed. Please ensure that the file path is correct and that the file is not corrupted. Full Error: {e}\")\n",
    "    \n",
    "    return text_by_page\n",
    "\n",
    "def chunk_input(input: list, n_chunks: int = 5) -> list[list]:\n",
    "    chunked_inputs = []\n",
    "\n",
    "    for idx in range(math.ceil(len(input) / n_chunks)):\n",
    "        start, end = idx * n_chunks, (idx+1) * n_chunks\n",
    "        chunked_inputs.append(input[start:end])\n",
    "\n",
    "    return chunked_inputs\n",
    "\n",
    "def get_file_ext(fpath: str) -> str:\n",
    "    return fpath.split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "615fb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text: str|list[str], target_word_ct: int, context_length: int = 8192) -> str:\n",
    "    summary_prompt = f\"You are an assistant that is tasked with summarizing a set of pages of documents that are given to you. Write a summary using the information provided. Do not use bullet points or any other formatting. The summary should be between {target_word_ct-50} and {target_word_ct+50} words long. The text is as follows: \"\n",
    "\n",
    "    if isinstance(input_text, str):\n",
    "        summary_prompt += input_text\n",
    "    elif isinstance(input_text, list) and all(isinstance(element, str) for element in input_text):\n",
    "        full_input = \"\\n\".join([chunk for chunk in input_text])\n",
    "        summary_prompt += full_input\n",
    "    else:\n",
    "        raise TypeError(\"The input text does not align with the intended types of `str` or `list[str]`. Please verify that your input text is either a string or a list of strings.\")\n",
    "\n",
    "    summary_response = generate(\n",
    "        model=MODEL_NAME,\n",
    "        prompt=summary_prompt,\n",
    "        options={\n",
    "            \"num_ctx\": context_length\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summary_response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fca1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunked_summaries(input_fpath: str) -> list[str]:\n",
    "    output_summaries = []\n",
    "\n",
    "    file_ext = get_file_ext(input_fpath)\n",
    "    if file_ext == \"pdf\":\n",
    "        parsed_input = process_pdf(input_fpath)\n",
    "    else:\n",
    "        raise ValueError(f\"Uploaded file type of `{file_ext}` is not supported for text parsing. Please try again without using any files with the specified file extension.\")\n",
    "    \n",
    "    summary_input = chunk_input(parsed_input)\n",
    "\n",
    "    if isinstance(summary_input, list) and len(summary_input) > 30:\n",
    "        n_chunk_groups = math.ceil(len(summary_input) / 30)\n",
    "\n",
    "        for chunk_group_idx in range(n_chunk_groups):\n",
    "            chunk_group_summaries = []\n",
    "\n",
    "            chunk_group_start_idx, chunk_group_end_idx = chunk_group_idx * n_chunk_groups, (chunk_group_idx + 1) * n_chunk_groups\n",
    "            chunk_group = summary_input[chunk_group_start_idx:chunk_group_end_idx]\n",
    "\n",
    "            for chunk in chunk_group:\n",
    "                chunk_summary = summarize(input_text=chunk, target_word_ct=200)\n",
    "                chunk_group_summaries.append(chunk_summary)\n",
    "            \n",
    "            chunk_group_summary = summarize(input_text=chunk_group_summaries, target_word_ct=500)\n",
    "            output_summaries.append(chunk_group_summary)\n",
    "\n",
    "    elif isinstance(summary_input, list) and len(summary_input) <= 30:\n",
    "        for chunk in summary_input:\n",
    "            chunk_summary = summarize(input_text=chunk, target_word_ct=200)\n",
    "            output_summaries.append(chunk_summary)\n",
    "\n",
    "    return output_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23c3a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create function to summarize chunk summaries into main summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7658ee13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Transformer, proposed by Vaswani et al., presents a novel neural network architecture for sequence transduction, eschewing recurrence and convolution entirely in favor of a self-attention mechanism. This paper details the model’s design, highlighting its superior performance and parallelizability compared to existing models like those utilizing recurrent neural networks or convolutional networks. The core innovation lies in the multi-head self-attention mechanism, allowing the model to draw global dependencies between input and output sequences. This architecture achieved state-of-the-art results on machine translation tasks, significantly improving translation quality while reducing training time.\\n\\nThe Transformer consists of stacked encoder and decoder layers, each incorporating multi-head self-attention and position-wise feed-forward networks. Crucially, the model utilizes scaled dot-product attention, mitigating issues with large dot product magnitudes and facilitating efficient computation. The multi-head attention component allows the model to attend to different representation subspaces at different positions, enhancing its ability to capture complex relationships within the data.\\n\\nThe paper emphasizes the model’s parallelizability, a significant advantage over sequential models like RNNs. This parallelizability, alongside the efficient scaling of attention computations, allowed the Transformer to reach a new state-of-the-art in translation quality with significantly reduced training costs, evidenced by a 41.8 BLEU score on English-to-French translation after just 3.5 days of training on eight P100 GPUs.  Furthermore, the model demonstrated versatility, successfully applying the self-attention mechanism to other tasks like English constituency parsing, with both large and limited training datasets. The authors detail the architecture, including the scaled dot-product attention, the use of multi-head attention, and the feed-forward networks.  They also provide a comprehensive explanation of the reasoning behind employing scaled dot product attention and the masking strategy to ensure the autoregressive nature of the decoder. Ultimately, the Transformer’s success stems from its ability to efficiently model long-range dependencies within sequences, leading to a substantial improvement in translation quality and a more efficient training process.\\n',\n",
       " \"The Transformer model, presented in this work, represents a significant advancement in sequence transduction, entirely based on the mechanism of self-attention. Unlike traditional encoder-decoder architectures that rely on recurrent layers, the Transformer eliminates these, achieving state-of-the-art results in machine translation. This paper details the design and training of the Transformer, focusing on its computational efficiency and ability to capture long-range dependencies within sequences.\\n\\nThe Transformer’s core innovation lies in its use of multi-headed self-attention. This allows the model to simultaneously attend to different parts of the input sequence, effectively capturing complex relationships between tokens. The model’s performance is highlighted through experiments on the WMT 2014 English-to-German and English-to-French translation tasks. Notably, the Transformer surpasses all previously reported models, including ensembles, achieving a new state-of-the-art BLEU score of 28.4 on English-to-German translation, and 41.0 on English-to-French.  The training process itself is remarkably efficient, requiring significantly less time than comparable recurrent or convolutional models.\\n\\nTo address the challenge of maintaining sequence order without recurrence, the Transformer incorporates “positional encodings.” These encodings, based on sine and cosine functions, inject information about the position of tokens within the sequence. The model’s ability to generalize to other tasks is then demonstrated through experiments on English constituency parsing.  The Transformer achieves results competitive with, and in some cases surpassing, state-of-the-art models, including those built using recurrent networks, further demonstrating its versatility.\\n\\nThe paper also details the training regime, utilizing a large dataset (WMT 2014) and employing techniques such as batching, Adam optimization, and dropout regularization.  The training process incorporates a warmup schedule and utilizes a beam search decoding strategy.  Crucially, the research explores various model variations, including changes to attention head size and key/value dimensions, to understand the impact of these parameters on performance. \\n\\nThe research concludes with a focus on the potential for future development, including extending the Transformer to handle multi-modal data and exploring methods to improve efficiency and scale. The code for the Transformer is publicly available, fostering further research and development in this exciting area of natural language processing. The model's training cost is significantly lower than competing models, highlighting a cost-effective solution for sequence transduction.\",\n",
       " 'This collection of research papers explores various advancements in neural machine translation and related areas of natural language processing. Several papers detail the development of recurrent neural networks (RNNs) and their application to sequence-to-sequence learning, particularly for translation tasks. Early work by Cho et al. (2014) focused on using RNN encoder-decoder models for translation, while Chung et al. (2014) evaluated gated recurrent neural networks. Subsequent research, including work by Dyer et al. (2016) and Luong et al. (2015), explored improvements to these models, such as grammatical structures and multi-task learning. The use of convolutional neural networks for translation is explored by Jonas Gehring et al. (2017), while others investigate techniques to handle rare words and improve accuracy. Furthermore, advancements in attention mechanisms and their integration with RNNs are examined, including implementations that utilize memory networks and deep residual learning. Many of these efforts focus on addressing the challenges of long-term dependencies, while also experimenting with various model architectures. Across these projects, key concepts like Adam optimization and subword units for handling rare words are investigated, demonstrating a consistent push toward higher accuracy and more efficient processing.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = create_chunked_summaries(\"../sample_inputs/transformers_paper.pdf\")\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
