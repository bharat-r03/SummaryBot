{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e27fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from ollama import chat, pull, generate\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34357206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n',\n",
       " '1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n',\n",
       " 'Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n',\n",
       " 'Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n',\n",
       " 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n',\n",
       " 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\n',\n",
       " 'length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\n',\n",
       " 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\n',\n",
       " 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\n',\n",
       " 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n',\n",
       " '[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n',\n",
       " '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\n',\n",
       " 'Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\n',\n",
       " 'The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\n',\n",
       " 'The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pymupdf.open(\"../../sample_inputs/transformers_paper.pdf\") as pdf:\n",
    "    text = [page.get_text() for page in pdf]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca51aa9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d3688",
   "metadata": {},
   "source": [
    "# Single Document w/ Chunking -- Llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a030f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull(\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4bd6ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_pages = []\n",
    "\n",
    "for i in range(math.ceil(len(text) // 5)):\n",
    "    start_idx, end_idx = i*5, (i+1)*5\n",
    "    chunk = text[start_idx:end_idx]\n",
    "    chunked_pages.append(\"\\n\".join([page for page in chunk]))\n",
    "\n",
    "chunked_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db92c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer proposes a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The model achieves state-of-the-art results in machine translation tasks while being more parallelizable and requiring significantly less time to train than existing models.\n",
      "The Transformer proposes a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The model achieves state-of-the-art results in machine translation tasks while being more parallelizable and requiring significantly less time to train than existing models.The Transformer is a novel sequence transduction model that relies exclusively on attention mechanisms, eliminating recurrent layers and convolutional operations from traditional architectures. By doing so, it offers significant advantages over its predecessors in terms of training speed, computational efficiency, and scalability. The proposed model architecture consists of two stacked sub-layers: self-attention and point-wise fully connected (ffn) layer for both the encoder and decoder.\n",
      "\n",
      "Self-attention allows all positions in a sequence to interact with each other simultaneously, whereas point-wise ffn layers process inputs sequentially. This arrangement enables parallelization of computations across different positions within a sequence, leading to substantial reductions in training time compared to existing models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
      "\n",
      "The Transformer also introduces positional encodings as an additional component for modeling the order or absolute position of tokens within a sequence. These encodings help the model recognize patterns without needing explicit recurrence, enabling it to tackle long-range dependencies effectively.\n",
      "\n",
      "Experimental results demonstrate that the Transformer outperforms state-of-the-art models in terms of BLEU scores on machine translation tasks, achieving record-breaking performance with substantial computational savings. Notably, our base model surpasses previous ensembles, while a larger version of the model significantly improves upon these results. Furthermore, we successfully adapt the Transformer to English constituency parsing tasks and show that it outperforms established benchmarks.\n",
      "\n",
      "Our findings suggest that attention-based models have great potential for advancing natural language processing tasks, particularly in translation and sequence generation. We plan to explore applications beyond text processing, investigating local attention mechanisms to efficiently handle large inputs such as images, audio, and video, and working towards making generation less sequential.\n",
      "The Transformer proposes a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The model achieves state-of-the-art results in machine translation tasks while being more parallelizable and requiring significantly less time to train than existing models.The Transformer is a novel sequence transduction model that relies exclusively on attention mechanisms, eliminating recurrent layers and convolutional operations from traditional architectures. By doing so, it offers significant advantages over its predecessors in terms of training speed, computational efficiency, and scalability. The proposed model architecture consists of two stacked sub-layers: self-attention and point-wise fully connected (ffn) layer for both the encoder and decoder.\n",
      "\n",
      "Self-attention allows all positions in a sequence to interact with each other simultaneously, whereas point-wise ffn layers process inputs sequentially. This arrangement enables parallelization of computations across different positions within a sequence, leading to substantial reductions in training time compared to existing models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
      "\n",
      "The Transformer also introduces positional encodings as an additional component for modeling the order or absolute position of tokens within a sequence. These encodings help the model recognize patterns without needing explicit recurrence, enabling it to tackle long-range dependencies effectively.\n",
      "\n",
      "Experimental results demonstrate that the Transformer outperforms state-of-the-art models in terms of BLEU scores on machine translation tasks, achieving record-breaking performance with substantial computational savings. Notably, our base model surpasses previous ensembles, while a larger version of the model significantly improves upon these results. Furthermore, we successfully adapt the Transformer to English constituency parsing tasks and show that it outperforms established benchmarks.\n",
      "\n",
      "Our findings suggest that attention-based models have great potential for advancing natural language processing tasks, particularly in translation and sequence generation. We plan to explore applications beyond text processing, investigating local attention mechanisms to efficiently handle large inputs such as images, audio, and video, and working towards making generation less sequential.The Transformer's architecture is designed to be highly parallelizable, allowing for significant computational efficiency gains compared to traditional models that rely on recurrent or convolutional layers. The model achieves state-of-the-art results in machine translation tasks while being more efficient in terms of training time and scalability. By leveraging self-attention mechanisms, the Transformer enables all positions in a sequence to interact with each other simultaneously, reducing the need for explicit recurrence and convolutional operations. Additionally, the incorporation of positional encodings allows the model to effectively tackle long-range dependencies without needing explicit recurrence. Experimental results demonstrate that the Transformer outperforms state-of-the-art models on machine translation tasks, achieving record-breaking performance while requiring substantial computational savings. Furthermore, the model is successfully adapted for English constituency parsing tasks, outperforming established benchmarks. The Transformer's architecture has significant implications for natural language processing tasks, particularly in translation and sequence generation, and its potential applications extend beyond text processing to other domains such as image, audio, and video analysis.\n"
     ]
    }
   ],
   "source": [
    "running_summary = \"\"\n",
    "\n",
    "for chunk in chunked_pages:\n",
    "    per_page_summary_prompt = f\"You are an assistant that is tasked with summarizing a set of documents that are given to you. The documents will be given in chunks, and you will be given the current summary. Do not rewrite the summary; just build on it. Do not use bullet points or formatting. Do not add any other text besides the summary. The text is as follows: {chunk}. The current summary is as follows: {running_summary}\"\n",
    "\n",
    "    response = generate(\n",
    "        model=\"llama3.2:3b\",\n",
    "        prompt=per_page_summary_prompt,\n",
    "        options={\n",
    "            \"num_ctx\": 8192\n",
    "        }\n",
    "    )\n",
    "\n",
    "    running_summary += response[\"response\"]\n",
    "    print(running_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "775c8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer proposes a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The model achieves state-of-the-art results in machine translation tasks while being more parallelizable and requiring significantly less time to train than existing models.The Transformer is a novel sequence transduction model that relies exclusively on attention mechanisms, eliminating recurrent layers and convolutional operations from traditional architectures. By doing so, it offers significant advantages over its predecessors in terms of training speed, computational efficiency, and scalability. The proposed model architecture consists of two stacked sub-layers: self-attention and point-wise fully connected (ffn) layer for both the encoder and decoder.\n",
      "\n",
      "Self-attention allows all positions in a sequence to interact with each other simultaneously, whereas point-wise ffn layers process inputs sequentially. This arrangement enables parallelization of computations across different positions within a sequence, leading to substantial reductions in training time compared to existing models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
      "\n",
      "The Transformer also introduces positional encodings as an additional component for modeling the order or absolute position of tokens within a sequence. These encodings help the model recognize patterns without needing explicit recurrence, enabling it to tackle long-range dependencies effectively.\n",
      "\n",
      "Experimental results demonstrate that the Transformer outperforms state-of-the-art models in terms of BLEU scores on machine translation tasks, achieving record-breaking performance with substantial computational savings. Notably, our base model surpasses previous ensembles, while a larger version of the model significantly improves upon these results. Furthermore, we successfully adapt the Transformer to English constituency parsing tasks and show that it outperforms established benchmarks.\n",
      "\n",
      "Our findings suggest that attention-based models have great potential for advancing natural language processing tasks, particularly in translation and sequence generation. We plan to explore applications beyond text processing, investigating local attention mechanisms to efficiently handle large inputs such as images, audio, and video, and working towards making generation less sequential.The Transformer's architecture is designed to be highly parallelizable, allowing for significant computational efficiency gains compared to traditional models that rely on recurrent or convolutional layers. The model achieves state-of-the-art results in machine translation tasks while being more efficient in terms of training time and scalability. By leveraging self-attention mechanisms, the Transformer enables all positions in a sequence to interact with each other simultaneously, reducing the need for explicit recurrence and convolutional operations. Additionally, the incorporation of positional encodings allows the model to effectively tackle long-range dependencies without needing explicit recurrence. Experimental results demonstrate that the Transformer outperforms state-of-the-art models on machine translation tasks, achieving record-breaking performance while requiring substantial computational savings. Furthermore, the model is successfully adapted for English constituency parsing tasks, outperforming established benchmarks. The Transformer's architecture has significant implications for natural language processing tasks, particularly in translation and sequence generation, and its potential applications extend beyond text processing to other domains such as image, audio, and video analysis.\n"
     ]
    }
   ],
   "source": [
    "print(running_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3edd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ae5b7",
   "metadata": {},
   "source": [
    "# Single Document w/ Chunking -- Gemma 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dbfd0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71ca68b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_pages = []\n",
    "\n",
    "for i in range(math.ceil(len(text) // 5)):\n",
    "    start_idx, end_idx = i*5, (i+1)*5\n",
    "    chunk = text[start_idx:end_idx]\n",
    "    chunked_pages.append(\"\\n\".join([page for page in chunk]))\n",
    "\n",
    "chunked_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c3c958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer, presented by Vaswani et al. (2017), represents a paradigm shift in sequence transduction, decisively abandoning the established recurrent and convolutional neural network paradigms. This meticulously detailed paper outlines the architecture and implementation of the Transformer, specifically engineered for machine translation, demonstrating a substantial leap in both translation quality and computational efficiency compared to prior state-of-the-art models. The core innovation lies in the utilization of scaled dot-product attention, multi-head attention, and positional encoding within a stacked encoder and decoder structure. The authors convincingly argue that this architecture elegantly overcomes the inherent limitations of traditional sequence transduction models, particularly concerning long-range dependencies and the challenges of parallelization. The Transformer achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing all previous models, including extensive ensembles, by over 2 BLEU points. On the WMT 2014 English-to-French translation task, the model establishes a new single-model BLEU score of 41.8 after training for 3.5 days on eight P100 GPUs – a fraction of the training costs of leading models. The paper highlights that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing, both with large and limited training datasets.\n",
      "\n",
      "The research begins by contextualizing the problem of sequence modeling and transduction, acknowledging the sequential nature of recurrent networks and the substantial difficulties in effectively capturing long-range dependencies. The authors systematically outline the design principles driving the Transformer’s architecture: the ability to efficiently parallelize computations and model complex relationships across the entire input sequence simultaneously. The Transformer’s architecture comprises a stack of N=6 identical encoder and decoder layers, each comprising multiple sub-layers. These sub-layers include multi-head self-attention and point-wise, fully connected feed-forward networks, all connected via residual connections and layer normalization. The authors meticulously explain how this arrangement facilitates both parallel computation and the capture of diverse contextual information.\n",
      "\n",
      "A critical component is the scaled dot-product attention mechanism, meticulously explained to mitigate the vanishing gradient problem—a major obstacle in training deep neural networks with long sequences. The scaling factor in the attention mechanism prevents gradients from becoming too small, allowing for effective training of deeper networks. The multi-head attention mechanism further enhances this process by enabling the model to jointly attend to different representation subspaces at various positions within the sequence, effectively capturing a broader range of contextual relationships. The paper clearly articulates how this parallelized attention allows the model to focus on the most relevant parts of the input sequence when generating each output token.\n",
      "\n",
      "Beyond simply describing the architecture, the paper provides a detailed account of the training process, encompassing learned embeddings, linear transformations, and a softmax function for predicting output tokens. The authors emphasize the importance of parameter sharing across layers, particularly the embedding layers, as a means of further enhancing training efficiency and reducing the parameter count. They also explain the rationale behind residual connections and layer normalization, highlighting their roles in stabilizing training and accelerating convergence. The Transformer’s architecture is inherently designed for parallelization, a crucial factor in significantly reducing training time compared to sequential models.\n",
      "\n",
      "The research team, led by Ashish Vaswani and incorporating the contributions of Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, and Illia Polosukhin, meticulously designed and implemented this groundbreaking model. The paper underscores the significance of the authors’ work, positioning the Transformer as a fundamental advancement in sequence transduction. Crucially, the paper references the 31st Conference on Neural Information Processing Systems (NIPS 2017) and the arXiv preprint version, establishing the Transformer’s impact within the broader research community. The core innovation of the Transformer lies in its ability to efficiently and accurately model complex relationships within sequences, opening up new possibilities for machine translation and other sequence-to-sequence tasks. The authors’ meticulous design, coupled with rigorous experimentation, convincingly confirms the Transformer as a definitive step forward in the field, demonstrating a new state-of-the-art in machine translation performance. The document’s thoroughness and clarity solidify the Transformer’s place as a seminal contribution, influencing subsequent research and development in the area of neural network architectures for sequence modeling. Furthermore, the paper includes a list of contributors, detailing each person's role within the project, acknowledging the collaborative effort behind this important research. The research further demonstrates the practical applicability of the Transformer by showcasing its successful application to English constituency parsing, showcasing its adaptability and generalizability.\n",
      "The Transformer architecture, as detailed in Vaswani et al.’s (2017) seminal paper, represents a revolutionary approach to sequence transduction, decisively moving away from the limitations of recurrent and convolutional neural networks. This meticulously crafted research meticulously outlines the design and implementation of the Transformer, specifically engineered for machine translation, achieving unprecedented translation quality and computational efficiency—significantly surpassing existing state-of-the-art models. The core innovation rests on the utilization of scaled dot-product attention, multi-head attention, and positional encoding within a stacked encoder and decoder structure. The authors convincingly demonstrate that this architecture elegantly addresses the inherent challenges of long-range dependencies and facilitates parallel computation, a crucial factor in dramatically reducing training time. The Transformer achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, exceeding all previous models, including extensive ensembles, by over 2 BLEU points.  Furthermore, on the WMT 2014 English-to-French translation task, the model establishes a new single-model BLEU score of 41.8, accomplished through training for 3.5 days on eight P100 GPUs – a fraction of the training costs associated with leading models.  The paper’s significance is amplified by its successful generalization to other tasks, notably English constituency parsing, both with large and limited training datasets, highlighting the Transformer’s adaptability and broad applicability.\n",
      "\n",
      "The research begins by contextualizing the persistent limitations of traditional sequence models, particularly recurrent networks’ struggles with capturing long-range dependencies and the computational bottlenecks associated with sequential processing. The authors systematically detail the design principles driving the Transformer’s architecture: its ability to efficiently parallelize computations and model intricate relationships across the entire input sequence simultaneously. The Transformer’s architecture consists of a stack of N=6 identical encoder and decoder layers, each comprising multiple sub-layers. These sub-layers include multi-head self-attention and point-wise, fully connected feed-forward networks, all connected via residual connections and layer normalization. The paper meticulously explains the rationale behind this arrangement—how it enables both parallel computation and the capture of diverse contextual information.\n",
      "\n",
      "A critical component is the scaled dot-product attention mechanism, expertly explained to mitigate the vanishing gradient problem—a major impediment to training deep neural networks with long sequences. The scaling factor within the attention mechanism prevents gradients from diminishing excessively, allowing for effective training of deeper networks. The multi-head attention mechanism further enhances this process by permitting the model to jointly attend to various representation subspaces at different positions within the sequence, effectively capturing a broader range of contextual relationships. The paper carefully articulates how this parallelized attention allows the model to focus on the most relevant parts of the input sequence when generating each output token.\n",
      "\n",
      "Beyond merely describing the architecture, the research provides a detailed account of the training process, encompassing learned embeddings, linear transformations, and a softmax function for predicting output tokens. The authors emphasize the importance of parameter sharing across layers, particularly the embedding layers, as a means of further enhancing training efficiency and reducing the parameter count.  They also clearly explain the rationale behind residual connections and layer normalization, highlighting their roles in stabilizing training and accelerating convergence. The Transformer’s architecture is inherently designed for parallelization, a crucial factor in significantly reducing training time compared to sequential models.\n",
      "\n",
      "The research team, spearheaded by Ashish Vaswani and incorporating the contributions of Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, and Illia Polosukhin, meticulously designed and implemented this groundbreaking model. The paper’s thoroughness and clarity solidify the Transformer’s place as a fundamental advancement in sequence transduction. Crucially, the document references the 31st Conference on Neural Information Processing Systems (NIPS 2017) and the arXiv preprint version, establishing the Transformer’s impact within the broader research community. The core innovation of the Transformer lies in its ability to efficiently and accurately model complex relationships within sequences, opening up new possibilities for machine translation and other sequence-to-sequence tasks. The research highlights the model’s contribution to the field of neural networks and showcases the innovative design choices that resulted in a significant performance boost. Furthermore, the paper includes a detailed list of contributors, acknowledging the collaborative effort behind this important research, underscoring the diverse expertise involved in the project’s development. The research further demonstrates the practical applicability of the Transformer by showcasing its successful application to English constituency parsing, highlighting its adaptability and generalizability, solidifying its position as a leading approach to sequence modeling.  The research’s significance extends beyond just improved translation performance—it represents a paradigm shift in how we approach sequence transduction problems.\n",
      "The Transformer architecture, as meticulously detailed in Vaswani et al.’s (2017) seminal paper, represents a revolutionary approach to sequence transduction, decisively moving away from the limitations of recurrent and convolutional neural networks. This thoroughly crafted research meticulously outlines the design and implementation of the Transformer, specifically engineered for machine translation, achieving unprecedented translation quality and computational efficiency—significantly surpassing existing state-of-the-art models. The core innovation rests on the utilization of scaled dot-product attention, multi-head attention, and positional encoding within a stacked encoder and decoder structure. The authors convincingly demonstrate that this architecture elegantly addresses the inherent challenges of long-range dependencies and facilitates parallel computation, a crucial factor in dramatically reducing training time. The Transformer achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, exceeding all previous models, including extensive ensembles, by over 2 BLEU points. Furthermore, on the WMT 2014 English-to-French translation task, the model establishes a new single-model BLEU score of 41.8, accomplished through training for 3.5 days on eight P100 GPUs – a fraction of the training costs associated with leading models. The paper’s significance is amplified by its successful generalization to other tasks, notably English constituency parsing, both with large and limited training datasets, highlighting the Transformer’s adaptability and broad applicability.\n",
      "\n",
      "The research begins by contextualizing the persistent limitations of traditional sequence models, particularly recurrent networks’ struggles with capturing long-range dependencies and the computational bottlenecks associated with sequential processing. The authors systematically detail the design principles driving the Transformer’s architecture: its ability to efficiently parallelize computations and model intricate relationships across the entire input sequence simultaneously. The Transformer’s architecture consists of a stack of N=6 identical encoder and decoder layers, each comprising multiple sub-layers. These sub-layers include multi-head self-attention and point-wise, fully connected feed-forward networks, all connected via residual connections and layer normalization. The paper meticulously explains the rationale behind this arrangement – how it enables both parallel computation and the capture of diverse contextual information.\n",
      "\n",
      "A critical component is the scaled dot-product attention mechanism, expertly explained to mitigate the vanishing gradient problem – a major impediment to training deep neural networks with long sequences. The scaling factor within the attention mechanism prevents gradients from diminishing excessively, allowing for effective training of deeper networks. The multi-head attention mechanism further enhances this process by permitting the model to jointly attend to various representation subspaces at different positions within the sequence, effectively capturing a broader range of contextual relationships. The paper carefully articulates how this parallelized attention allows the model to focus on the most relevant parts of the input sequence when generating each output token.\n",
      "\n",
      "Beyond merely describing the architecture, the research provides a detailed account of the training process, encompassing learned embeddings, linear transformations, and a softmax function for predicting output tokens. The authors emphasize the importance of parameter sharing across layers, particularly the embedding layers, as a means of further enhancing training efficiency and reducing the parameter count. They also clearly explain the rationale behind residual connections and layer normalization, highlighting their roles in stabilizing training and accelerating convergence. The Transformer’s architecture is inherently designed for parallelization, a crucial factor in dramatically reducing training time compared to sequential models.\n",
      "\n",
      "The research team, spearheaded by Ashish Vaswani and incorporating the contributions of Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, and Illia Polosukhin, meticulously designed and implemented this groundbreaking model. The paper’s thoroughness and clarity solidify the Transformer’s place as a fundamental advancement in sequence transduction. Crucially, the document references the 31st Conference on Neural Information Processing Systems (NIPS 2017) and the arXiv preprint version, establishing the Transformer’s impact within the broader research community. The core innovation of the Transformer lies in its ability to efficiently and accurately model complex relationships within sequences, opening up new possibilities for machine translation and other sequence-to-sequence tasks. The research highlights the model’s contribution to the field of neural networks and showcases the innovative design choices that resulted in a significant performance boost. Furthermore, the paper includes a detailed list of contributors, acknowledging the collaborative effort behind this important research, underscoring the diverse expertise involved in the project’s development. The research further demonstrates the practical applicability of the Transformer by showcasing its successful application to English constituency parsing, highlighting its adaptability and generalizability, solidifying its position as a leading approach to sequence modeling. The study’s findings powerfully demonstrate the potential of the Transformer architecture to revolutionize sequence transduction, marking a significant milestone in the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "running_summary = \"\"\n",
    "\n",
    "for chunk in chunked_pages:\n",
    "    per_page_summary_prompt = f\"You are an assistant that is tasked with summarizing a set of documents that are given to you. The documents will be given in chunks, and you will be given the current summary. Write a summary using the information provided. Do not use bullet points or any other formatting. The summary should be at least 500 words long. The text is as follows: {chunk}. The current summary is as follows: {response['response']}\"\n",
    "\n",
    "    response = generate(\n",
    "        model=\"gemma3:4b\",\n",
    "        prompt=per_page_summary_prompt,\n",
    "        options={\n",
    "            \"num_ctx\": 8192\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "394d6a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture, as meticulously detailed in Vaswani et al.’s (2017) seminal paper, represents a revolutionary approach to sequence transduction, decisively moving away from the limitations of recurrent and convolutional neural networks. This thoroughly crafted research meticulously outlines the design and implementation of the Transformer, specifically engineered for machine translation, achieving unprecedented translation quality and computational efficiency—significantly surpassing existing state-of-the-art models. The core innovation rests on the utilization of scaled dot-product attention, multi-head attention, and positional encoding within a stacked encoder and decoder structure. The authors convincingly demonstrate that this architecture elegantly addresses the inherent challenges of long-range dependencies and facilitates parallel computation, a crucial factor in dramatically reducing training time. The Transformer achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, exceeding all previous models, including extensive ensembles, by over 2 BLEU points. Furthermore, on the WMT 2014 English-to-French translation task, the model establishes a new single-model BLEU score of 41.8, accomplished through training for 3.5 days on eight P100 GPUs – a fraction of the training costs associated with leading models. The paper’s significance is amplified by its successful generalization to other tasks, notably English constituency parsing, both with large and limited training datasets, highlighting the Transformer’s adaptability and broad applicability.\n",
      "\n",
      "The research begins by contextualizing the persistent limitations of traditional sequence models, particularly recurrent networks’ struggles with capturing long-range dependencies and the computational bottlenecks associated with sequential processing. The authors systematically detail the design principles driving the Transformer’s architecture: its ability to efficiently parallelize computations and model intricate relationships across the entire input sequence simultaneously. The Transformer’s architecture consists of a stack of N=6 identical encoder and decoder layers, each comprising multiple sub-layers. These sub-layers include multi-head self-attention and point-wise, fully connected feed-forward networks, all connected via residual connections and layer normalization. The paper meticulously explains the rationale behind this arrangement – how it enables both parallel computation and the capture of diverse contextual information.\n",
      "\n",
      "A critical component is the scaled dot-product attention mechanism, expertly explained to mitigate the vanishing gradient problem – a major impediment to training deep neural networks with long sequences. The scaling factor within the attention mechanism prevents gradients from diminishing excessively, allowing for effective training of deeper networks. The multi-head attention mechanism further enhances this process by permitting the model to jointly attend to various representation subspaces at different positions within the sequence, effectively capturing a broader range of contextual relationships. The paper carefully articulates how this parallelized attention allows the model to focus on the most relevant parts of the input sequence when generating each output token.\n",
      "\n",
      "Beyond merely describing the architecture, the research provides a detailed account of the training process, encompassing learned embeddings, linear transformations, and a softmax function for predicting output tokens. The authors emphasize the importance of parameter sharing across layers, particularly the embedding layers, as a means of further enhancing training efficiency and reducing the parameter count. They also clearly explain the rationale behind residual connections and layer normalization, highlighting their roles in stabilizing training and accelerating convergence. The Transformer’s architecture is inherently designed for parallelization, a crucial factor in dramatically reducing training time compared to sequential models.\n",
      "\n",
      "The research team, spearheaded by Ashish Vaswani and incorporating the contributions of Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, and Illia Polosukhin, meticulously designed and implemented this groundbreaking model. The paper’s thoroughness and clarity solidify the Transformer’s place as a fundamental advancement in sequence transduction. Crucially, the document references the 31st Conference on Neural Information Processing Systems (NIPS 2017) and the arXiv preprint version, establishing the Transformer’s impact within the broader research community. The core innovation of the Transformer lies in its ability to efficiently and accurately model complex relationships within sequences, opening up new possibilities for machine translation and other sequence-to-sequence tasks. The research highlights the model’s contribution to the field of neural networks and showcases the innovative design choices that resulted in a significant performance boost. Furthermore, the paper includes a detailed list of contributors, acknowledging the collaborative effort behind this important research, underscoring the diverse expertise involved in the project’s development. The research further demonstrates the practical applicability of the Transformer by showcasing its successful application to English constituency parsing, highlighting its adaptability and generalizability, solidifying its position as a leading approach to sequence modeling. The study’s findings powerfully demonstrate the potential of the Transformer architecture to revolutionize sequence transduction, marking a significant milestone in the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bad953",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff881870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Deep Reinforcement Learning that Matters\\nPeter Henderson1∗, Riashat Islam1,2∗, Philip Bachman2\\nJoelle Pineau1, Doina Precup1, David Meger1\\n1 McGill University, Montreal, Canada\\n2 Microsoft Maluuba, Montreal, Canada\\n{peter.henderson,riashat.islam}@mail.mcgill.ca, phbachma@microsoft.com\\n{jpineau,dprecup}@cs.mcgill.ca, dmeger@cim.mcgill.ca\\nAbstract\\nIn recent years, signiﬁcant progress has been made in solving\\nchallenging problems across various domains using deep re-\\ninforcement learning (RL). Reproducing existing work and\\naccurately judging the improvements offered by novel meth-\\nods is vital to sustaining this progress. Unfortunately, repro-\\nducing results for state-of-the-art deep RL methods is seldom\\nstraightforward. In particular, non-determinism in standard\\nbenchmark environments, combined with variance intrinsic\\nto the methods, can make reported results tough to interpret.\\nWithout signiﬁcance metrics and tighter standardization of\\nexperimental reporting, it is difﬁcult to determine whether im-\\nprovements over the prior state-of-the-art are meaningful. In\\nthis paper, we investigate challenges posed by reproducibility,\\nproper experimental techniques, and reporting procedures. We\\nillustrate the variability in reported metrics and results when\\ncomparing against common baselines and suggest guidelines\\nto make future results in deep RL more reproducible. We aim\\nto spur discussion about how to ensure continued progress in\\nthe ﬁeld by minimizing wasted effort stemming from results\\nthat are non-reproducible and easily misinterpreted.\\nIntroduction\\nReinforcement learning (RL) is the study of how an agent\\ncan interact with its environment to learn a policy which\\nmaximizes expected cumulative rewards for a task. Recently,\\nRL has experienced dramatic growth in attention and interest\\ndue to promising results in areas like: controlling continuous\\nsystems in robotics (Lillicrap et al. 2015a), playing Go (Silver\\net al. 2016), Atari (Mnih et al. 2013), and competitive video\\ngames (Vinyals et al. 2017; Silva and Chaimowicz 2017).\\nFigure 1 illustrates growth of the ﬁeld through the number\\nof publications per year. To maintain rapid progress in RL\\nresearch, it is important that existing works can be easily\\nreproduced and compared to accurately judge improvements\\noffered by novel methods.\\nHowever, reproducing deep RL results is seldom straight-\\nforward, and the literature reports a wide range of results\\nfor the same baseline algorithms (Islam et al. 2017). Re-\\nproducibility can be affected by extrinsic factors (e.g. hy-\\nperparameters or codebases) and intrinsic factors (e.g. ef-\\n∗These two authors contributed equally\\nCopyright c⃝2018, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\nFigure 1: Growth of published reinforcement learning papers.\\nShown are the number of RL-related publications (y-axis)\\nper year (x-axis) scraped from Google Scholar searches.\\nfects of random seeds or environment properties). We inves-\\ntigate these sources of variance in reported results through\\na representative set of experiments. For clarity, we focus\\nour investigation on policy gradient (PG) methods in con-\\ntinuous control. Policy gradient methods with neural net-\\nwork function approximators have been particularly suc-\\ncessful in continuous control (Schulman et al. 2015a; 2017;\\nLillicrap et al. 2015b) and are competitive with value-based\\nmethods in discrete settings. We note that the diversity of\\nmetrics and lack of signiﬁcance testing in the RL literature\\ncreates the potential for misleading reporting of results. We\\ndemonstrate possible beneﬁts of signiﬁcance testing using\\ntechniques common in machine learning and statistics.\\nSeveral works touch upon evaluating RL algorithms. Duan\\net al. (2016) benchmark several RL algorithms and provide\\nthe community with baseline implementations. Generaliz-\\nable RL evaluation metrics are proposed in (Whiteson et al.\\n2011). Machado et al. (2017) revisit the Arcade Learning\\nEnvironment to propose better evaluation methods in these\\nbenchmarks. However, while the question of reproducibility\\nand good experimental practice has been examined in related\\nﬁelds (Wagstaff 2012; Boulesteix, Lauer, and Eugster 2013;\\nStodden, Leisch, and Peng 2014; Bouckaert and Frank 2004;\\nBouckaert 2004; Vaughan and Wawerla 2012), to the best of\\nour knowledge this is the ﬁrst work to address this important\\nquestion in the context of deep RL.\\nIn each section of our experimental analysis, we pose ques-\\ntions regarding key factors affecting reproducibility. We ﬁnd\\nthat there are numerous sources of non-determinism when\\nreproducing and comparing RL algorithms. To this end, we\\nshow that ﬁne details of experimental procedure can be crit-\\narXiv:1709.06560v3  [cs.LG]  30 Jan 2019\\n',\n",
       "  'ical. Based on our experiments, we conclude with possible\\nrecommendations, lines of investigation, and points of dis-\\ncussion for future works to ensure that deep reinforcement\\nlearning is reproducible and continues to matter.\\nTechnical Background\\nThis work focuses on several model-free policy gradient\\nalgorithms with publicly available implementations which\\nappear frequently in the literature as baselines for compar-\\nison against novel methods. We experiment with Trust Re-\\ngion Policy Optimization (TRPO) (Schulman et al. 2015a),\\nDeep Deterministic Policy Gradients (DDPG) (Lillicrap et\\nal. 2015b), Proximal Policy Optimization (PPO) (Schulman\\net al. 2017), and Actor Critic using Kronecker-Factored\\nTrust Region (ACKTR) (Wu et al. 2017). These methods\\nhave shown promising results in continuous control MuJoCo\\ndomain tasks (Todorov, Erez, and Tassa 2012) from Ope-\\nnAI Gym (Brockman et al. 2016). Generally, they optimize\\nρ(θ, s0) = Eπθ [P∞\\nt=0 γtr(st)|s0], using the policy gradient\\ntheorem:\\nδρ(θ,s0)\\nδθ\\n= P\\ns µπθ(s|s0) P\\na\\nδπθ(a|s)\\nδθ\\nQπθ(s, a).\\nHere, µπθ(s|s0) = P∞\\nt=0 γtP(st = s|s0) (Sutton et al.\\n2000). TRPO (Schulman et al. 2015a) and PPO (Schulman\\net al. 2017) use constraints and advantage estimation to per-\\nform this update, reformulating the optimization problem\\nas: maxθ Et\\nh\\nπθ(at|st)\\nπθold(at|st)At(st, at)\\ni\\n. Here, At is the general-\\nized advantage function (Schulman et al. 2015b). TRPO uses\\nconjugate gradient descent as the optimization method with\\na KL constraint: Et [KL [πθold(·|st), πθ(·|st)]] ≤δ. PPO re-\\nformulates the constraint as a penalty (or clipping objective).\\nDDPG and ACKTR use actor-critic methods which estimate\\nQ(s, a) and optimize a policy that maximizes the Q-function\\nbased on Monte-Carlo rollouts. DDPG does this using deter-\\nministic policies, while ACKTR uses Kronecketer-factored\\ntrust regions to ensure stability with stochastic policies.\\nExperimental Analysis\\nWe pose several questions about the factors affecting repro-\\nducibility of state-of-the-art RL methods. We perform a set\\nof experiments designed to provide insight into the questions\\nposed. In particular, we investigate the effects of: speciﬁc\\nhyperparameters on algorithm performance if not properly\\ntuned; random seeds and the number of averaged experi-\\nment trials; speciﬁc environment characteristics; differences\\nin algorithm performance due to stochastic environments;\\ndifferences due to codebases with most other factors held\\nconstant. For most of our experiments1, except for those com-\\nparing codebases, we generally use the OpenAI Baselines2\\nimplementations of the following algorithms: ACKTR (Wu\\net al. 2017), PPO (Schulman et al. 2017), DDPG (Plappert et\\nal. 2017), TRPO (Schulman et al. 2017). We use the Hopper-\\nv1 and HalfCheetah-v1 MuJoCo (Todorov, Erez, and Tassa\\n2012) environments from OpenAI Gym (Brockman et al.\\n2016). These two environments provide contrasting dynam-\\nics (the former being more unstable).\\n1Speciﬁc details can be found in the supplemental and code can\\nbe found at: https://git.io/vFHnf\\n2https://www.github.com/openai/baselines\\nTo ensure fairness we run ﬁve experiment trials for each\\nevaluation, each with a different preset random seed (all\\nexperiments use the same set of random seeds). In all cases,\\nwe highlight important results here, with full descriptions of\\nexperimental setups and additional learning curves included\\nin the supplemental material. Unless otherwise mentioned,\\nwe use default settings whenever possible, while modifying\\nonly the hyperparameters of interest. All results (including\\ngraphs) show mean and standard error across random seeds.\\nWe use multilayer perceptron function approximators in\\nall cases. We denote the hidden layer sizes and activations\\nas (N, M, activation). For default settings, we vary the hy-\\nperparameters under investigation one at a time. For DDPG\\nwe use a network structure of (64, 64, ReLU) for both actor\\nand critic. For TRPO and PPO, we use (64, 64, tanh) for the\\npolicy. For ACKTR, we use (64, 64, tanh) for the actor and\\n(64, 64, ELU) for the critic.\\nHyperparameters\\nWhat is the magnitude of the effect hyperparameter settings\\ncan have on baseline performance?\\nTuned hyperparameters play a large role in eliciting the best\\nresults from many algorithms. However, the choice of op-\\ntimal hyperparameter conﬁguration is often not consistent\\nin related literature, and the range of values considered is\\noften not reported3. Furthermore, poor hyperparameter selec-\\ntion can be detrimental to a fair comparison against baseline\\nalgorithms. Here, we investigate several aspects of hyperpa-\\nrameter selection on performance.\\nNetwork Architecture\\nHow does the choice of network architecture for the policy\\nand value function approximation affect performance?\\nIn (Islam et al. 2017), it is shown that policy network architec-\\nture can signiﬁcantly impact results in both TRPO and DDPG.\\nFurthermore, certain activation functions such as Rectiﬁed\\nLinear Unit (ReLU) have been shown to cause worsened\\nlearning performance due to the “dying relu” problem (Xu et\\nal. 2015). As such, we examine network architecture and ac-\\ntivation functions for both policy and value function approxi-\\nmators. In the literature, similar lines of investigation have\\nshown the differences in performance when comparing linear\\napproximators, RBFs, and neural networks (Rajeswaran et\\nal. 2017). Tables 1 and 2 summarize the ﬁnal evaluation per-\\nformance of all architectural variations after training on 2M\\nsamples (i.e. 2M timesteps in the environment). All learning\\ncurves and details on setup can be found in the supplemental\\nmaterial. We vary hyperparameters one at a time, while using\\na default setting for all others. We investigate three multilayer\\nperceptron (MLP) architectures commonly seen in the liter-\\nature: (64, 64), (100, 50, 25), and (400, 300). Furthermore,\\nwe vary the activation functions of both the value and policy\\nnetworks across tanh, ReLU, and Leaky ReLU activations.\\nResults Figure 2 shows how signiﬁcantly performance can\\nbe affected by simple changes to the policy or value network\\n3A sampled literature review can be found in the supplemental.\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−2000\\n−1000\\n0\\n1000\\n2000\\nAverage Return\\nHalfCheetah-v1 (PPO, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−750\\n−500\\n−250\\n0\\n250\\n500\\n750\\n1000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Policy Network Activation)\\ntanh\\nrelu\\nleaky relu\\nFigure 2: Signiﬁcance of Policy Network Structure and Activation Functions PPO (left), TRPO (middle) and DDPG (right).\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Reward Scale, Layer Norm)\\nrs=1e-4\\nrs=1e-3\\nrs=1e-2\\nrs=1e-1\\nrs=1\\nrs=10\\nrs=100\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Reward Scale, No Layer Norm)\\nrs=1e-4\\nrs=1e-3\\nrs=1e-2\\nrs=1e-1\\nrs=1\\nrs=10\\nrs=100\\nFigure 3: DDPG reward rescaling on HalfCheetah-v1, with and without layer norm.\\nactivations. We ﬁnd that usually ReLU or Leaky ReLU acti-\\nvations perform the best across environments and algorithms.\\nThe effects are not consistent across algorithms or environ-\\nments. This inconsistency demonstrates how interconnected\\nnetwork architecture is to algorithm methodology. For exam-\\nple, using a large network with PPO may require tweaking\\nother hyperparameters such as the trust region clipping or\\nlearning rate to compensate for the architectural change4.\\nThis intricate interplay of hyperparameters is one of the rea-\\nsons reproducing current policy gradient methods is so dif-\\nﬁcult. It is exceedingly important to choose an appropriate\\narchitecture for proper baseline results. This also suggests a\\npossible need for hyperparameter agnostic algorithms—that\\nis algorithms that incorporate hyperparameter adaptation as\\npart of the design—such that fair comparisons can be made\\nwithout concern about improper settings for the task at hand.\\nReward Scale\\nHow can the reward scale affect results? Why is reward\\nrescaling used?\\nReward rescaling has been used in several recent works\\n(Duan et al. 2016; Gu et al. 2016) to improve results for\\nDDPG. This involves simply multiplying the rewards gen-\\nerated from an environment by some scalar (ˆr = rˆσ) for\\ntraining. Often, these works report using a reward scale\\nof ˆσ = 0.1. In Atari domains, this is akin to clipping the\\nrewards to [0, 1]. By intuition, in gradient based methods\\n(as used in most deep RL) a large and sparse output scale\\ncan result in problems regarding saturation and inefﬁciency\\nin learning (LeCun et al. 2012; Glorot and Bengio 2010;\\nVincent, de Br´ebisson, and Bouthillier 2015). Therefore clip-\\nping or rescaling rewards compresses the space of estimated\\n4We ﬁnd that the KL divergence of updates with the large net-\\nwork (400, 300) seen in Figure 2 is on average 33.52 times higher\\nthan the KL divergence of updates with the (64, 64) network.\\nexpected returns in action value function based methods such\\nas DDPG. We run a set of experiments using reward rescaling\\nin DDPG (with and without layer normalization) for insights\\ninto how this aspect affects performance.\\nResults Our analysis shows that reward rescaling can have\\na large effect (full experiment results can be found in the\\nsupplemental material), but results were inconsistent across\\nenvironments and scaling values. Figure 3 shows one such ex-\\nample where reward rescaling affects results, causing a failure\\nto learn in small settings below ˆσ = 0.01. In particular, layer\\nnormalization changes how the rescaling factor affects results,\\nsuggesting that these impacts are due to the use of deep net-\\nworks and gradient-based methods. With the value function\\napproximator tracking a moving target distribution, this can\\npotentially affect learning in unstable environments where\\na deep Q-value function approximator is used. Furthermore,\\nsome environments may have untuned reward scales (e.g.\\nthe HumanoidStandup-v1 of OpenAI gym which can reach\\nrewards in the scale of millions). Therefore, we suggest that\\nthis hyperparameter has the potential to have a large impact\\nif considered properly. Rather than rescaling rewards in some\\nenvironments, a more principled approach should be taken\\nto address this. An initial foray into this problem is made\\nin (van Hasselt et al. 2016), where the authors adaptively\\nrescale reward targets with normalized stochastic gradient,\\nbut further research is needed.\\nRandom Seeds and Trials\\nCan random seeds drastically alter performance? Can one\\ndistort results by averaging an improper number of trials?\\nA major concern with deep RL is the variance in results due\\nto environment stochasticity or stochasticity in the learning\\nprocess (e.g. random weight initialization). As such, even\\naveraging several learning results together across totally dif-\\nferent random seeds can lead to the reporting of misleading\\nresults. We highlight this in the form of an experiment.\\n',\n",
       "  'Algorithm\\nEnvironment\\n400,300\\n64,64\\n100,50,25\\ntanh\\nReLU\\nLeakyReLU\\nTRPO\\nHopper-v1\\n2980 ± 35\\n2674 ± 227\\n3110 ± 78\\n2674 ± 227\\n2772 ± 211\\n-\\n(Schulman et al. 2015a)\\nHalfCheetah-v1\\n1791 ± 224\\n1939 ± 140\\n2151 ± 27\\n1939 ± 140\\n3041 ± 161\\n-\\nTRPO\\nHopper-v1\\n1243 ± 55\\n1303 ± 89\\n1243 ± 55\\n1303 ± 89\\n1131 ± 65\\n1341± 127\\n(Duan et al. 2016)\\nHalfCheetah-v1\\n738 ± 240\\n834 ± 317\\n850±378\\n834 ± 317\\n784 ± 352\\n1139 ±364\\nTRPO\\nHopper-v1\\n2909 ± 87\\n2828 ± 70\\n2812 ± 88\\n2828 ± 70\\n2941 ± 91\\n2865 ± 189\\n(Schulman et al. 2017)\\nHalfCheetah-v1\\n-155 ± 188\\n205 ± 256\\n306 ± 261\\n205 ± 256\\n1045 ± 114\\n778 ± 177\\nPPO\\nHopper-v1\\n61 ± 33\\n2790 ± 62\\n2592 ± 196\\n2790 ± 62\\n2695 ± 86\\n2587 ± 53\\n(Schulman et al. 2017)\\nHalfCheetah-v1\\n-1180 ± 444\\n2201 ± 323\\n1314 ± 340\\n2201 ± 323\\n2971 ± 364\\n2895 ± 365\\nDDPG\\nHopper-v1\\n1419 ± 313\\n1632 ± 459\\n2142 ± 436\\n1491 ± 205\\n1632 ± 459\\n1384 ± 285\\n(Plappert et al. 2017)\\nHalfCheetah-v1\\n5579 ± 354\\n4198 ± 606\\n5600 ± 601\\n5325 ± 281\\n4198 ± 606\\n4094 ± 233\\nDDPG\\nHopper-v1\\n600 ± 126\\n593 ± 155\\n501 ± 129\\n436 ± 48\\n593 ± 155\\n319 ± 127\\n(Gu et al. 2016)\\nHalfCheetah-v1\\n2845 ± 589\\n2771 ± 535\\n1638 ± 624\\n1638 ± 624\\n2771 ± 535\\n1405± 511\\nDDPG\\nHopper-v1\\n506 ± 208\\n749 ± 271\\n629 ± 138\\n354 ± 91\\n749 ± 271\\n-\\n(Duan et al. 2016)\\nHalfCheetah-v1\\n850 ± 41\\n1573 ± 385\\n1224 ± 553\\n1311 ± 271\\n1573 ± 385\\n-\\nACKTR\\nHopper-v1\\n2577 ± 529\\n1608 ± 66\\n2287 ± 946\\n1608 ± 66\\n2835 ± 503\\n2718 ± 434\\n(Wu et al. 2017)\\nHalfCheetah-v1\\n2653 ± 408\\n2691 ± 231\\n2498 ± 112\\n2621 ± 381\\n2160 ± 151\\n2691 ± 231\\nTable 1: Results for our policy architecture permutations across various implementations and algorithms. Final average ±\\nstandard error across 5 trials of returns across the last 100 trajectories after 2M training samples. For ACKTR, we use ELU\\nactivations instead of leaky ReLU.\\nAlgorithm\\nEnvironment\\n400,300\\n64,64\\n100,50,25\\ntanh\\nReLU\\nLeakyReLU\\nTRPO\\nHopper-v1\\n3011 ± 171\\n2674 ± 227\\n2782 ± 120\\n2674 ± 227\\n3104 ± 84\\n-\\n(Schulman et al. 2015a)\\nHalfCheetah-v1\\n2355 ± 48\\n1939 ± 140\\n1673 ± 148\\n1939 ± 140\\n2281 ± 91\\n-\\nTRPO\\nHopper-v1\\n2909 ± 87\\n2828 ± 70\\n2812 ± 88\\n2828 ± 70\\n2829 ± 76\\n3047 ± 68\\n(Schulman et al. 2017)\\nHalfCheetah-v1\\n178 ± 242\\n205 ± 256\\n172 ± 257\\n205 ± 256\\n235 ± 260\\n325 ± 208\\nPPO\\nHopper-v1\\n2704 ± 37\\n2790 ± 62\\n2969 ± 111\\n2790 ± 62\\n2687 ± 144\\n2748 ± 77\\n(Schulman et al. 2017)\\nHalfCheetah-v1\\n1523 ± 297\\n2201 ± 323\\n1807 ± 309\\n2201 ± 323\\n1288 ± 12\\n1227 ± 462\\nDDPG\\nHopper-v1\\n1419 ± 312\\n1632 ± 458\\n1569 ± 453\\n971 ± 137\\n852 ± 143\\n843 ± 160\\n(Plappert et al. 2017)\\nHalfCheetah-v1\\n5600 ± 601\\n4197 ± 606\\n4713 ± 374\\n3908 ± 293\\n4197 ± 606\\n5324 ± 280\\nDDPG\\nHopper-v1\\n523 ± 248\\n343 ± 34\\n345 ± 44\\n436 ± 48\\n343 ± 34\\n-\\n(Gu et al. 2016)\\nHalfCheetah-v1\\n1373 ± 678\\n1717 ± 508\\n1868 ± 620\\n1128 ± 511\\n1717 ± 508\\n-\\nDDPG\\nHopper-v1\\n1208 ± 423\\n394 ± 144\\n380 ± 65\\n354 ± 91\\n394 ± 144\\n-\\n(Duan et al. 2016)\\nHalfCheetah-v1\\n789 ± 91\\n1095 ± 139\\n988 ± 52\\n1311 ± 271\\n1095 ± 139\\n-\\nACKTR\\nHopper-v1\\n152 ± 47\\n1930 ± 185\\n1589 ± 225\\n691 ± 55\\n500 ± 379\\n1930 ± 185\\n(Wu et al. 2017)\\nHalfCheetah-v1\\n518 ± 632\\n3018 ± 386\\n2554 ± 219\\n2547 ± 172\\n3362 ± 682\\n3018 ± 38\\nTable 2: Results for our value function (Q or V ) architecture permutations across various implementations and algorithms. Final\\naverage ± standard error across 5 trials of returns across the last 100 trajectories after 2M training samples. For ACKTR, we use\\nELU activations instead of leaky ReLU.\\nFigure 4: Performance of several policy gradient algorithms across benchmark MuJoCo environment suites\\nEnvironment\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nHalfCheetah-v1\\n5037 (3664, 6574)\\n3888 (2288, 5131)\\n1254.5 (999, 1464)\\n3043 (1920, 4165)\\nHopper-v1\\n1632 (607, 2370)\\n2546 (1875, 3217)\\n2965 (2854, 3076)\\n2715 (2589, 2847)\\nWalker2d-v1\\n1582 (901, 2174)\\n2285 (1246, 3235)\\n3072 (2957, 3183)\\n2926 (2514, 3361)\\nSwimmer-v1\\n31 (21, 46)\\n50 (42, 55)\\n214 (141, 287)\\n107 (101, 118)\\nTable 3: Bootstrap mean and 95% conﬁdence bounds for a subset of environment experiments. 10k bootstrap iterations and the\\npivotal method were used.\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Diﬀerent Random Seeds)\\nRandom Average (5 runs)\\nRandom Average (5 runs)\\nFigure 5: TRPO on HalfCheetah-v1 using the same hyperpa-\\nrameter conﬁgurations averaged over two sets of 5 different\\nrandom seeds each. The average 2-sample t-test across entire\\ntraining distribution resulted in t = −9.0916, p = 0.0016.\\nResults We perform 10 experiment trials, for the same\\nhyperparameter conﬁguration, only varying the random seed\\nacross all 10 trials. We then split the trials into two sets of\\n5 and average these two groupings together. As shown in\\nFigure 5, we ﬁnd that the performance of algorithms can\\nbe drastically different. We demonstrate that the variance\\nbetween runs is enough to create statistically different dis-\\ntributions just from varying random seeds. Unfortunately, in\\nrecent reported results, it is not uncommon for the top-N tri-\\nals to be selected from among several trials (Wu et al. 2017;\\nMnih et al. 2016) or averaged over only small number of tri-\\nals (N < 5) (Gu et al. 2017; Wu et al. 2017). Our experiment\\nwith random seeds shows that this can be potentially mislead-\\ning. Particularly for HalfCheetah, it is possible to get learning\\ncurves that do not fall within the same distribution at all, just\\nby averaging different runs with the same hyperparameters,\\nbut different random seeds. While there can be no speciﬁc\\nnumber of trials speciﬁed as a recommendation, it is possible\\nthat power analysis methods can be used to give a general\\nidea to this extent as we will discuss later. However, more\\ninvestigation is needed to answer this open problem.\\nEnvironments\\nHow do the environment properties affect variability in re-\\nported RL algorithm performance?\\nTo assess how the choice of evaluation environment can af-\\nfect the presented results, we use our aforementioned default\\nset of hyperparameters across our chosen testbed of algo-\\nrithms and investigate how well each algorithm performs\\nacross an extended suite of continuous control tasks. For\\nthese experiments, we use the following environments from\\nOpenAI Gym: Hopper-v1, HalfCheetah-v1, Swimmer-v1 and\\nWalker2d-v1. The choice of environment often plays an im-\\nportant role in demonstrating how well a new proposed algo-\\nrithm performs against baselines. In continuous control tasks,\\noften the environments have random stochasticity, shortened\\ntrajectories, or different dynamic properties. We demonstrate\\nthat, as a result of these differences, algorithm performance\\ncan vary across environments and the best performing algo-\\nrithm across all environments is not always clear. Thus it is\\nincreasingly important to present results for a wide range of\\nenvironments and not only pick those which show a novel\\nwork outperforming other methods.\\nResults As shown in Figure 4, in environments with sta-\\nble dynamics (e.g. HalfCheetah-v1), DDPG outperforms all\\nother algorithsm. However, as dynamics become more unsta-\\nble (e.g. in Hopper-v1) performance gains rapidly diminish.\\nAs DDPG is an off-policy method, exploration noise can\\ncause sudden failures in unstable environments. Therefore,\\nlearning a proper Q-value estimation of expected returns is\\ndifﬁcult, particularly since many exploratory paths will result\\nin failure. Since failures in such tasks are characterized by\\nshortened trajectories, a local optimum in this case would be\\nsimply to survive until the maximum length of the trajectory\\n(corresponding to one thousand timesteps and similar reward\\ndue to a survival bonus in the case of Hopper-v1). As can be\\nseen in Figure 4, DDPG with Hopper does exactly this. This\\nis a clear example where showing only the favourable and sta-\\nble HalfCheetah when reporting DDPG-based experiments\\nwould be unfair.\\nFurthermore, let us consider the Swimmer-v1 environment\\nshown in Figure 4. Here, TRPO signiﬁcantly outperforms\\nall other algorithms. Due to the dynamics of the water-like\\nenvironment, a local optimum for the system is to curl up and\\nﬂail without proper swimming. However, this corresponds\\nto a return of ∼130. By reaching a local optimum, learning\\ncurves can indicate successful optimization of the policy over\\ntime, when in reality the returns achieved are not qualitatively\\nrepresentative of learning the desired behaviour, as demon-\\nstrated in video replays of the learned policy5. Therefore,\\nit is important to show not only returns but demonstrations\\nof the learned policy in action. Without understanding what\\nthe evaluation returns indicate, it is possible that misleading\\nresults can be reported which in reality only optimize local\\noptima rather than reaching the desired behaviour.\\nCodebases\\nAre commonly used baseline implementations comparable?\\nIn many cases, authors implement their own versions of base-\\nline algorithms to compare against. We investigate the Ope-\\nnAI baselines implementation of TRPO as used in (Schulman\\net al. 2017), the original TRPO code (Schulman et al. 2015a),\\nand the rllab (Duan et al. 2016) Tensorﬂow implementation of\\nTRPO. We also compare the rllab Theano (Duan et al. 2016),\\nrllabplusplus (Gu et al. 2016), and OpenAI baselines (Plap-\\npert et al. 2017) implementations of DDPG. Our goal is to\\ndraw attention to the variance due to implementation details\\nacross algorithms. We run a subset of our architecture experi-\\nments as with the OpenAI baselines implementations using\\nthe same hyperparameters as in those experiments6.\\nResults We ﬁnd that implementation differences which\\nare often not reﬂected in publications can have dramatic\\nimpacts on performance. This can be seen for our ﬁnal evalu-\\nation performance after training on 2M samples in Tables 1\\nand 2, as well as a sample comparison in Figure 6. This\\n5https://youtu.be/lKpUQYjgm80\\n6Differences are discussed in the supplemental (e.g. use of dif-\\nferent optimizers for the value function baseline). Leaky ReLU\\nactivations are left out to narrow the experiment scope.\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Codebase Comparison)\\nSchulman 2015\\nSchulman 2017\\nDuan 2016\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Codebase Comparison)\\nDuan 2016\\nGu 2016\\nPlapper 2017\\nFigure 6: TRPO codebase comparison using our default set\\nof hyperparameters (as used in other experiments).\\ndemonstrates the necessity that implementation details be\\nenumerated, codebases packaged with publications, and that\\nperformance of baseline experiments in novel works matches\\nthe original baseline publication code.\\nReporting Evaluation Metrics\\nIn this section we analyze some of the evaluation metrics\\ncommonly used in the reinforcement learning literature. In\\npractice, RL algorithms are often evaluated by simply pre-\\nsenting plots or tables of average cumulative reward (average\\nreturns) and, more recently, of maximum reward achieved\\nover a ﬁxed number of timesteps. Due to the unstable na-\\nture of many of these algorithms, simply reporting the max-\\nimum returns is typically inadequate for fair comparison;\\neven reporting average returns can be misleading as the range\\nof performance across seeds and trials is unknown. Alone,\\nthese may not provide a clear picture of an algorithm’s range\\nof performance. However, when combined with conﬁdence\\nintervals, this may be adequate to make an informed deci-\\nsion given a large enough number of trials. As such, we\\ninvestigate using the bootstrap and signiﬁcance testing as in\\nML (Kohavi and others 1995; Bouckaert and Frank 2004;\\nNadeau and Bengio 2000) to evaluate algorithm performance.\\nOnline View vs. Policy Optimization An important dis-\\ntinction when reporting results is the online learning view\\nversus the policy optimization view of RL. In the online view,\\nan agent will optimize the returns across the entire learning\\nprocess and there is not necessarily an end to the agent’s\\ntrajectory. In this view, evaluations can use the average cumu-\\nlative rewards across the entire learning process (balancing\\nexploration and exploitation) as in (Hofer and Gimbert 2016),\\nor can possibly use ofﬂine evaluation as in (Mandel et al.\\n2016). The alternate view corresponds to policy optimization,\\nwhere evaluation is performed using a target policy in an of-\\nﬂine manner. In the policy optimization view it is important to\\nrun evaluations across the entire length of the task trajectory\\nwith a single target policy to determine the average returns\\nthat the target can obtain. We focus on evaluation methods\\nfor the policy optimization view (with ofﬂine evaluation), but\\nthe same principles can be applied to the online view.\\nConﬁdence Bounds The sample bootstrap has been a pop-\\nular method to gain insight into a population distribution\\nfrom a smaller sample (Efron and Tibshirani 1994). Boot-\\nstrap methods are particularly popular for A/B testing, and\\nwe can borrow some ideas from this ﬁeld. Generally a boot-\\nstrap estimator is obtained by resampling with replacement\\nmany times to generate a statistically relevant mean and con-\\nﬁdence bound. Using this technique, we can gain insight into\\nwhat is the 95% conﬁdence interval of the results from our\\nsection on environments. Table 3 shows the bootstrap mean\\nand 95% conﬁdence bounds on our environment experiments.\\nConﬁdence intervals can vary wildly between algorithms and\\nenvironments. We ﬁnd that TRPO and PPO are the most\\nstable with small conﬁdence bounds from the bootstrap. In\\ncases where conﬁdence bounds are exceedingly large, it may\\nbe necessary to run more trials (i.e. increase the sample size).\\nPower Analysis Another method to determine if the\\nsample size must be increased is bootstrap power analy-\\nsis (Tuff´ery 2011; Yuan and Hayashi 2003). If we use our\\nsample and give it some uniform lift (for example, scaling uni-\\nformly by 1.25), we can run many bootstrap simulations and\\ndetermine what percentage of the simulations result in statis-\\ntically signiﬁcant values with the lift. If there is a small per-\\ncentage of signiﬁcant values, a larger sample size is needed\\n(more trials must be run). We do this across all environment\\nexperiment trial runs and indeed ﬁnd that, in more unstable\\nsettings, the bootstrap power percentage leans towards in-\\nsigniﬁcant results in the lift experiment. Conversely, in stable\\ntrials (e.g. TRPO on Hopper-v1) with a small sample size,\\nthe lift experiment shows that no more trials are needed to\\ngenerate signiﬁcant comparisons. These results are provided\\nin the supplemental material.\\nSigniﬁcance An important factor when deciding on an\\nRL algorithm to use is the signiﬁcance of the reported gains\\nbased on a given metric. Several works have investigated\\nthe use of signiﬁcance metrics to assess the reliability of\\nreported evaluation metrics in ML. However, few works in\\nreinforcement learning assess the signiﬁcance of reported\\nmetrics. Based on our experimental results which indicate\\nthat algorithm performance can vary wildly based simply on\\nperturbations of random seeds, it is clear that some metric is\\nnecessary for assessing the signiﬁcance of algorithm perfor-\\nmance gains and the conﬁdence of reported metrics. While\\nmore research and investigation is needed to determine the\\nbest metrics for assessing RL algorithms, we investigate an\\ninitial set of metrics based on results from ML.\\nIn supervised learning, k-fold t-test, corrected resampled t-\\ntest, and other signiﬁcance metrics have been discussed when\\ncomparing machine learning results (Bouckaert and Frank\\n2004; Nadeau and Bengio 2000). However, the assumptions\\npertaining to the underlying data with corrected metrics do\\nnot necessarily apply in RL. Further work is needed to inves-\\ntigate proper corrected signiﬁcance tests for RL. Nonetheless,\\nwe explore several signiﬁcance measures which give insight\\n',\n",
       "  'into whether a novel algorithm is truly performing as the state-\\nof-the-art. We consider the simple 2-sample t-test (sorting all\\nﬁnal evaluation returns across N random trials with different\\nrandom seeds); the Kolmogorov-Smirnov test (Wilcox 2005);\\nand bootstrap percent differences with 95% conﬁdence in-\\ntervals. All calculated metrics can be found in the supple-\\nmental. Generally, we ﬁnd that the signiﬁcance values match\\nup to what is to be expected. Take, for example, comparing\\nWalker2d-v1 performance of ACKTR vs. DDPG. ACKTR\\nperforms slightly better, but this performance is not signiﬁ-\\ncant due to the overlapping conﬁdence intervals of the two:\\nt = 1.03, p = 0.334, KS = 0.40, p = 0.697, bootstrapped\\npercent difference 44.47% (-80.62%, 111.72%).\\nDiscussion and Conclusion\\nThrough experimental methods focusing on PG methods\\nfor continuous control, we investigate problems with repro-\\nducibility in deep RL. We ﬁnd that both intrinsic (e.g. random\\nseeds, environment properties) and extrinsic sources (e.g. hy-\\nperparameters, codebases) of non-determinism can contribute\\nto difﬁculties in reproducing baseline algorithms. Moreover,\\nwe ﬁnd that highly varied results due to intrinsic sources\\nbolster the need for using proper signiﬁcance analysis. We\\npropose several such methods and show their value on a\\nsubset of our experiments.\\nWhat recommendations can we draw from our experiments?\\nBased on our experimental results and investigations, we\\ncan provide some general recommendations. Hyperparame-\\nters can have signiﬁcantly different effects across algorithms\\nand environments. Thus it is important to ﬁnd the work-\\ning set which at least matches the original reported perfor-\\nmance of baseline algorithms through standard hyperparame-\\nter searches. Similarly, new baseline algorithm implementa-\\ntions used for comparison should match the original codebase\\nresults if available. Overall, due to the high variance across\\ntrials and random seeds of reinforcement learning algorithms,\\nmany trials must be run with different random seeds when\\ncomparing performance. Unless random seed selection is\\nexplicitly part of the algorithm, averaging multiple runs over\\ndifferent random seeds gives insight into the population dis-\\ntribution of the algorithm performance on an environment.\\nSimilarly, due to these effects, it is important to perform\\nproper signiﬁcance testing to determine if the higher average\\nreturns are in fact representative of better performance.\\nWe highlight several forms of signiﬁcance testing and ﬁnd\\nthat they give generally expected results when taking conﬁ-\\ndence intervals into consideration. Furthermore, we demon-\\nstrate that bootstrapping and power analysis are possible ways\\nto gain insight into the number of trial runs necessary to make\\nan informed decision about the signiﬁcance of algorithm per-\\nformance gains. In general, however, the most important step\\nto reproducibility is to report all hyperparameters, implemen-\\ntation details, experimental setup, and evaluation methods for\\nboth baseline comparison methods and novel work. Without\\nthe publication of implementations and related details, wasted\\neffort on reproducing state-of-the-art works will plague the\\ncommunity and slow down progress.\\nWhat are possible future lines of investigation?\\nDue to the signiﬁcant effects of hyperparameters (partic-\\nularly reward scaling), another possibly important line of\\nfuture investigation is in building hyperparameter agnostic\\nalgorithms. Such an approach would ensure that there is no\\nunfairness introduced from external sources when compar-\\ning algorithms agnostic to parameters such as reward scale,\\nbatch size, or network structure. Furthermore, while we in-\\nvestigate an initial set of signiﬁcance metrics here, they may\\nnot be the best ﬁt for comparing RL algorithms. Several\\nworks have begun investigating policy evaluation methods\\nfor the purposes of safe RL (Thomas and Brunskill 2016;\\nThomas, Theocharous, and Ghavamzadeh 2015), but further\\nwork is needed in signiﬁcance testing and statistical analysis.\\nSimilar lines of investigation to (Nadeau and Bengio 2000;\\nBouckaert and Frank 2004) would be helpful to determine the\\nbest methods for evaluating performance gain signiﬁcance.\\nHow can we ensure that deep RL matters?\\nWe discuss many different factors affecting reproducibility of\\nRL algorithms. The sensitivity of these algorithms to changes\\nin reward scale, environment dynamics, and random seeds\\ncan be considerable and varies between algorithms and set-\\ntings. Since benchmark environments are proxies for real-\\nworld applications to gauge generalized algorithm perfor-\\nmance, perhaps more emphasis should be placed on the appli-\\ncability of RL algorithms to real-world tasks. That is, as there\\nis often no clear winner among all benchmark environments,\\nperhaps recommended areas of application should be demon-\\nstrated along with benchmark environment results when pre-\\nsenting a new algorithm. Maybe new methods should be\\nanswering the question: in what setting would this work be\\nuseful? This is something that is addressed for machine learn-\\ning in (Wagstaff 2012) and may warrant more discussion for\\nRL. As a community, we must not only ensure reproducible\\nresults with fair comparisons, but we must also consider what\\nare the best ways to demonstrate that RL continues to matter.\\nAcknowledgements\\nWe thank NSERC, CIFAR, the Open Philanthropy Project,\\nand the AWS Cloud Credits for Research Program.\\nReferences\\nBouckaert, R. R., and Frank, E. 2004. Evaluating the replicability\\nof signiﬁcance tests for comparing learning algorithms. In PAKDD,\\n3–12. Springer.\\nBouckaert, R. R. 2004. Estimating replicability of classiﬁer learning\\nexperiments. In Proceedings of the 21st International Conference\\non Machine Learning (ICML).\\nBoulesteix, A.-L.; Lauer, S.; and Eugster, M. J. 2013. A plea for\\nneutral comparison studies in computational sciences. PloS one\\n8(4):e61562.\\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman,\\nJ.; Tang, J.; and Zaremba, W. 2016. OpenAI gym. arXiv preprint\\narXiv:1606.01540.\\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel, P.\\n2016. Benchmarking deep reinforcement learning for continuous\\ncontrol. In Proceedings of the 33rd International Conference on\\nMachine Learning (ICML).\\n',\n",
       "  'Efron, B., and Tibshirani, R. J. 1994. An introduction to the boot-\\nstrap. CRC press.\\nGlorot, X., and Bengio, Y. 2010. Understanding the difﬁculty of\\ntraining deep feedforward neural networks. In Proceedings of the\\nThirteenth International Conference on Artiﬁcial Intelligence and\\nStatistics, 249–256.\\nGu, S.; Lillicrap, T.; Ghahramani, Z.; Turner, R. E.; and Levine, S.\\n2016. Q-prop: Sample-efﬁcient policy gradient with an off-policy\\ncritic. arXiv preprint arXiv:1611.02247.\\nGu, S.; Lillicrap, T.; Ghahramani, Z.; Turner, R. E.; Sch¨olkopf, B.;\\nand Levine, S. 2017. Interpolated policy gradient: Merging on-\\npolicy and off-policy gradient estimation for deep reinforcement\\nlearning. arXiv preprint arXiv:1706.00387.\\nHofer, L., and Gimbert, H. 2016. Online reinforcement learning for\\nreal-time exploration in continuous state and action markov decision\\nprocesses. arXiv preprint arXiv:1612.03780.\\nIslam, R.; Henderson, P.; Gomrokchi, M.; and Precup, D. 2017.\\nReproducibility of benchmarked deep reinforcement learning tasks\\nfor continuous control. ICML Reproducibility in Machine Learning\\nWorkshop.\\nKohavi, R., et al. 1995. A study of cross-validation and bootstrap\\nfor accuracy estimation and model selection. In IJCAI, volume 14.\\nLeCun, Y. A.; Bottou, L.; Orr, G. B.; and M¨uller, K.-R. 2012. Efﬁ-\\ncient backprop. In Neural Networks: Tricks of the Trade. Springer.\\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.;\\nSilver, D.; and Wierstra, D. 2015a. Continuous control with deep\\nreinforcement learning. arXiv preprint arXiv:1509.02971.\\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.;\\nSilver, D.; and Wierstra, D. 2015b. Continuous control with deep\\nreinforcement learning. arXiv preprint arXiv:1509.02971.\\nMachado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.;\\nHausknecht, M.; and Bowling, M. 2017. Revisiting the arcade\\nlearning environment: Evaluation protocols and open problems for\\ngeneral agents. arXiv preprint arXiv:1709.06009.\\nMandel, T.; Liu, Y.-E.; Brunskill, E.; and Popovic, Z. 2016. Ofﬂine\\nEvaluation of Online Reinforcement Learning Algorithms. In AAAI.\\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.;\\nWierstra, D.; and Riedmiller, M. 2013. Playing atari with deep\\nreinforcement learning. arXiv preprint arXiv:1312.5602.\\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley,\\nT.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous methods\\nfor deep reinforcement learning. In International Conference on\\nMachine Learning, 1928–1937.\\nNadeau, C., and Bengio, Y. 2000. Inference for the generalization\\nerror. In Advances in neural information processing systems.\\nPlappert, M.; Houthooft, R.; Dhariwal, P.; Sidor, S.; Chen, R.; Chen,\\nX.; Asfour, T.; Abbeel, P.; and Andrychowicz, M. 2017. Parameter\\nspace noise for exploration. arXiv preprint arXiv:1706.01905.\\nRajeswaran, A.; Lowrey, K.; Todorov, E.; and Kakade, S. 2017.\\nTowards generalization and simplicity in continuous control. arXiv\\npreprint arXiv:1703.02660.\\nSchulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz, P.\\n2015a. Trust region policy optimization. In Proceedings of the 32nd\\nInternational Conference on Machine Learning (ICML).\\nSchulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P.\\n2015b. High-dimensional continuous control using generalized\\nadvantage estimation. arXiv preprint arXiv:1506.02438.\\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov,\\nO. 2017. Proximal policy optimization algorithms. arXiv preprint\\narXiv:1707.06347.\\nSilva, V. d. N., and Chaimowicz, L. 2017. Moba: a new arena for\\ngame ai. arXiv preprint arXiv:1705.10443.\\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van\\nDen Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershel-\\nvam, V.; Lanctot, M.; et al. 2016. Mastering the game of go with\\ndeep neural networks and tree search. Nature 529(7587):484–489.\\nStadie, B. C.; Abbeel, P.; and Sutskever, I. 2017. Third-person\\nimitation learning. arXiv preprint arXiv:1703.01703.\\nStodden, V.; Leisch, F.; and Peng, R. D. 2014. Implementing\\nreproducible research. CRC Press.\\nSutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y.\\n2000. Policy gradient methods for reinforcement learning with func-\\ntion approximation. In Advances in neural information processing\\nsystems.\\nThomas, P., and Brunskill, E. 2016. Data-efﬁcient off-policy policy\\nevaluation for reinforcement learning. In International Conference\\non Machine Learning, 2139–2148.\\nThomas, P. S.; Theocharous, G.; and Ghavamzadeh, M. 2015. High-\\nConﬁdence Off-Policy Evaluation. In AAAI.\\nTodorov, E.; Erez, T.; and Tassa, Y. 2012. Mujoco: A physics engine\\nfor model-based control. In 2012 IEEE/RSJ International Confer-\\nence on Intelligent Robots and Systems, IROS 2012, Vilamoura,\\nAlgarve, Portugal, October 7-12, 2012, 5026–5033.\\nTuff´ery, S. 2011. Data mining and statistics for decision making,\\nvolume 2. Wiley Chichester.\\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\\nD. 2016. Learning values across many orders of magnitude. In\\nAdvances in Neural Information Processing Systems, 4287–4295.\\nVaughan, R., and Wawerla, J. 2012. Publishing identiﬁable exper-\\niment code and conﬁguration is important, good and easy. arXiv\\npreprint arXiv:1204.2235.\\nVincent, P.; de Br´ebisson, A.; and Bouthillier, X. 2015. Efﬁcient\\nexact gradient update for training deep networks with very large\\nsparse targets. In Advances in Neural Information Processing Sys-\\ntems, 1108–1116.\\nVinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets,\\nA. S.; Yeo, M.; Makhzani, A.; K¨uttler, H.; Agapiou, J.; Schrittwieser,\\nJ.; et al. 2017. Starcraft ii: A new challenge for reinforcement\\nlearning. arXiv preprint arXiv:1708.04782.\\nWagstaff, K. 2012. Machine learning that matters. arXiv preprint\\narXiv:1206.4656.\\nWhiteson, S.; Tanner, B.; Taylor, M. E.; and Stone, P. 2011. Pro-\\ntecting against evaluation overﬁtting in empirical reinforcement\\nlearning. In 2011 IEEE Symposium on Adaptive Dynamic Program-\\nming And Reinforcement Learning, ADPRL 2011, Paris, France,\\nApril 12-14, 2011, 120–127.\\nWilcox, R. 2005. Kolmogorov–smirnov test. Encyclopedia of\\nbiostatistics.\\nWu, Y.; Mansimov, E.; Liao, S.; Grosse, R.; and Ba, J. 2017. Scal-\\nable trust-region method for deep reinforcement learning using\\nkronecker-factored approximation. arXiv preprint:1708.05144.\\nXu, B.; Wang, N.; Chen, T.; and Li, M. 2015. Empirical evaluation\\nof rectiﬁed activations in convolutional network. arXiv preprint\\narXiv:1505.00853.\\nYuan, K.-H., and Hayashi, K. 2003. Bootstrap approach to inference\\nand power analysis based on three test statistics for covariance\\nstructure models. British Journal of Mathematical and Statistical\\nPsychology 56(1):93–110.\\n',\n",
       "  'Supplemental Material\\nIn this supplemental material, we include a detailed review of experiment conﬁgurations of related work with policy gradient methods in\\ncontinuous control MuJoCo (Todorov, Erez, and Tassa 2012) environment tasks from OpenAI Gym (Brockman et al. 2016). We include\\na detailed list of the hyperparameters and reported metrics typically used in policy gradient literature in deep RL. We also include all our\\nexperimental results, with baseline algorithms DDPG (Lillicrap et al. 2015b), TRPO (Schulman et al. 2015a), PPO (Schulman et al. 2017)\\nand ACKTR (Wu et al. 2017)) as discussed in the paper. Our experimental results include ﬁgures with different hyperparameters (network\\narchitectures, activation functions) to highlight the differences this can have across algorithms and environments. Finally, as discussed in the\\npaper, we include discussion of signiﬁcance metrics and show how these metrics can be useful for evaluating deep RL algorithms.\\nLiterature Reviews\\nHyperparameters\\nIn this section, we include a list of hyperparameters that are reported in related literature, as shown in ﬁgure 4. Our analysis shows that often\\nthere is no consistency in the type of network architectures and activation functions that are used in related literature. As shown in the paper and\\nfrom our experimental results in later sections, we ﬁnd, however, that these hyperparameters can have a signiﬁcant effect in the performance of\\nalgorithms across benchmark environments typically used.\\nTable 4: Evaluation Hyperparameters of baseline algorithms reported in related literature\\nRelated Work\\n(Algorithm)\\nPolicy\\nNetwork\\nPolicy\\nNetwork\\nActivation\\nValue\\nNetwork\\nValue\\nNetwork\\nActivation\\nReward\\nScaling\\nBatch\\nSize\\nDDPG\\n64x64\\nReLU\\n64x64\\nReLU\\n1.0\\n128\\nTRPO\\n64x64\\nTanH\\n64x64\\nTanH\\n-\\n5k\\nPPO\\n64x64\\nTanH\\n64x64\\nTanH\\n-\\n2048\\nACKTR\\n64x64\\nTanH\\n64x64\\nELU\\n-\\n2500\\nQ-Prop\\n(DDPG)\\n100x50x25\\nTanH\\n100x100\\nReLU\\n0.1\\n64\\nQ-Prop\\n(TRPO)\\n100x50x25\\nTanH\\n100x100\\nReLU\\n-\\n5k\\nIPG\\n(TRPO)\\n100x50x25\\nTanH\\n100x100\\nReLU\\n-\\n10k\\nParam Noise\\n(DDPG)\\n64x64\\nReLU\\n64x64\\nReLU\\n-\\n128\\nParam Noise\\n(TRPO)\\n64x64\\nTanH\\n64x64\\nTanH\\n-\\n5k\\nBenchmarking\\n(DDPG)\\n400x300\\nReLU\\n400x300\\nReLU\\n0.1\\n64\\nBenchmarking\\n(TRPO)\\n100x50x25\\nTanH\\n100x50x25\\nTanH\\n-\\n25k\\nReported Results on Benchmarked Environments\\nWe then demonstrate how experimental reported results, on two different environments (HalfCheetah-v1 and Hopper-v1) can vary across\\ndifferent related work that uses these algorithms for baseline comparison. We further show the results we get, using the same hyperparameter\\nconﬁguration, but using two different codebase implementations (note that these implementations are often used as baseline codebase to\\ndevelop algorithms). We highlight that, depending on the codebase used, experimental results can vary signiﬁcantly.\\nTable 5: Comparison with Related Reported Results with Hopper Environment\\nEnvironment\\nMetric\\nrllab\\nQProp\\nIPG\\nTRPO\\nOur Results\\n(rllab)\\nOur Results\\n(Baselines)\\nTRPO on\\nHopper\\nEnvironment\\nNumber of Iterations\\n500\\n500\\n500\\n500\\n500\\n500\\nAverage Return\\n1183.3\\n-\\n-\\n-\\n2021.34\\n2965.3\\nMax Average Return\\n-\\n2486\\n3668.8\\n3229.1\\n3034.4\\n',\n",
       "  'Table 6: Comparison with Related Reported Results with HalfCheetah Environment\\nEnvironment\\nMetric\\nrllab\\nQProp\\nIPG\\nTRPO\\nOur Results\\n(rllab)\\nOur Results\\n(Baselines)\\nTRPO on\\nHalfCheetah\\nEnvironment\\nNumber of Iterations\\n500\\n500\\n500\\n500\\n500\\n500\\nAverage Return\\n1914.0\\n-\\n-\\n3576.08\\n1045.6\\nMax Average Return\\n-\\n4734\\n2889\\n4855\\n5197\\n1045.6\\nWork\\nNumber of Trials\\n(Mnih et al. 2016)\\ntop-5\\n(Schulman et al. 2017)\\n3-9\\n(Duan et al. 2016)\\n5 (5)\\n(Gu et al. 2017)\\n3\\n(Lillicrap et al. 2015b)\\n5\\n(Schulman et al. 2015a)\\n5\\n(Wu et al. 2017)\\ntop-2, top-3\\nTable 7: Number of trials reported during evaluation in various works.\\nReported Evaluation Metrics in Related Work\\nIn table 8 we show the evaluation metrics, and reported results in further details across related work.\\nTable 8: Reported Evaluation Metrics of baseline algorithms in related literature\\nRelated Work\\n(Algorithm)\\nEnvironments\\nTimesteps\\nor Episodes\\nor Iterations\\nEvaluation Metrics\\nAverage\\nReturn\\nMax\\nReturn\\nStd\\nError\\nPPO\\nHalfCheetah\\nHopper\\n1M\\n∼1800\\n∼2200\\n-\\n-\\nACKTR\\nHalfCheetah\\nHopper\\n1M\\n∼2400\\n∼3500\\n-\\n-\\nQ-Prop\\n(DDPG)\\nHalfCheetah\\nHopper\\n6k (eps)\\n∼6000\\n-\\n7490\\n2604\\n-\\n-\\nQ-Prop\\n(TRPO)\\nHalfCheetah\\nHopper\\n5k (timesteps)\\n∼4000\\n-\\n4734\\n2486\\n-\\n-\\nIPG\\n(TRPO)\\nHalfCheetah\\nHopper\\n10k (eps)\\n∼3000\\n-\\n2889\\n-\\n-\\nParam Noise\\n(DDPG)\\nHalfCheetah\\nHopper\\n1M\\n∼1800\\n∼500\\n-\\n-\\n-\\n-\\nParam Noise\\n(TRPO)\\nHalfCheetah\\nHopper\\n1M\\n∼3900\\n∼2400\\n-\\n-\\n-\\n-\\nBenchmarking\\n(DDPG)\\nHalfCheetah\\nHopper\\n500 iters\\n(25k eps)\\n∼2148\\n∼267\\n-\\n-\\n702\\n43\\nBenchmarking\\n(TRPO)\\nHalfCheetah\\nHopper\\n500 iters\\n(925k eps)\\n∼1914\\n∼1183\\n-\\n-\\n150\\n120\\nExperimental Setup\\nIn this section, we show detailed analysis of our experimental results, using same hyperparameter conﬁgurations used in related work.\\nExperimental results are included for the OpenAI Gym (Brockman et al. 2016) Hopper-v1 and HalfCheetah-v1 environments, using the\\npolicy gradient algorithms including DDPG, TRPO, PPO and ACKTR. Our experiments are done using the available codebase from OpenAI\\nrllab (Duan et al. 2016) and OpenAI Baselines. Each of our experiments are performed over 5 experimental trials with different random\\nseeds, and results averaged over all trials. Unless explicitly speciﬁed as otherwise (such as in hyperparameter modiﬁcations where we alter a\\nhyperparameter under investigation), hyperparameters were as follows. All results (including graphs) show mean and standard error across\\nrandom seeds.\\n• DDPG\\n',\n",
       "  '– Policy Network: (64, relu, 64, relu, tanh); Q Network (64, relu, 64, relu, linear)\\n– Normalized observations with running mean ﬁlter\\n– Actor LR: 1e −4; Critic LR: 1e −3\\n– Reward Scale: 1.0\\n– Noise type: O-U 0.2\\n– Soft target update τ = .01\\n– γ = 0.995\\n– batch size = 128\\n– Critic L2 reg 1e −2\\n• PPO\\n– Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, tanh, 64, tanh, linear)\\n– Normalized observations with running mean ﬁlter\\n– Timesteps per batch 2048\\n– clip param = 0.2\\n– entropy coeff = 0.0\\n– Optimizer epochs per iteration = 10\\n– Optimizer step size 3e −4\\n– Optimizer batch size 64\\n– Discount γ = 0.995, GAE λ = 0.97\\n– learning rate schedule is constant\\n• TRPO\\n– Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, tanh, 64, tanh, linear)\\n– Normalized observations with running mean ﬁlter\\n– Timesteps per batch 5000\\n– max KL=0.01\\n– Conjugate gradient iterations = 20\\n– CG damping = 0.1\\n– VF Iterations = 5\\n– VF Batch Size = 64\\n– VF Step Size = 1e −3\\n– entropy coeff = 0.0\\n– Discount γ = 0.995, GAE λ = 0.97\\n• ACKTR\\n– Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, elu, 64, elu, linear)\\n– Normalized observations with running mean ﬁlter\\n– Timesteps per batch 2500\\n– desired KL = .002\\n– Discount γ = 0.995, GAE λ = 0.97\\nModiﬁcations to Baseline Implementations\\nTo ensure fairness of comparison, we make several modiﬁcations to the existing implementations. First, we change evaluation in DDPG (Plappert\\net al. 2017) such that during evaluation at the end of an epoch, 10 full trajectories are evaluated. In the current implementation, only a partial\\ntrajectory is evaluated immediately after training such that a full trajectory will be evaluated across several different policies, this corresponds\\nmore closely to the online view of evaluation, while we take a policy optimization view when evaluating algorithms.\\nHyperparameters : Network Structures and Activation Functions\\nBelow, we examine the signiﬁcance of the network conﬁgurations used for the non-linear function approximators in policy gradient methods.\\nSeveral related work have used different sets of network conﬁgurations (network sizes and activation functions). We use the reported network\\nconﬁgurations from other works, and demonstrate the signiﬁcance of careful ﬁne tuning that is required. We demonstrate results using the\\nnetwork activation functions, ReLU, TanH and Leaky ReLU, where most papers use ReLU and TanH as activation functions without detailed\\nreporting of the effect of these activation functions. We analyse the signifcance of using different activations in the policy and action value\\nnetworks. Previously, we included a detailed table showing average reward with standard error obtained for each of the hyperparameter\\nconﬁgurations. In the results below, we show detailed results of how each of these policy gradient algorithms are affected by the choice of the\\nnetwork conﬁguration.\\n',\n",
       "  'Proximal Policy Optimization (PPO)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\nAverage Return\\nHalfCheetah-v1 (PPO, Value Network Activation)\\ntanh\\nrelu\\nleaky relu\\nFigure 7: PPO Policy and Value Network activation\\nExperiment results in Figure 7, 8, and 9 in this section show the effect of the policy network structures and activation functions in the Proximal\\nPolicy Optimization (PPO) algorithm.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−2000\\n−1000\\n0\\n1000\\n2000\\nAverage Return\\nHalfCheetah-v1 (PPO, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\nFigure 8: PPO Policy Network structure\\n',\n",
       "  'Figure 9: PPO Value Network structure\\nActor Critic using Kronecker-Factored Trust Region (ACKTR)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHalfCheetah-v1 (ACKTR, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (ACKTR, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\nFigure 10: ACKTR Policy Network structure\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHalfCheetah-v1 (ACKTR, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (ACKTR, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\nFigure 11: ACKTR Value Network structure\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHalfCheetah-v1 (ACKTR, Policy Network Activation)\\ntanh\\nrelu\\nelu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (ACKTR, Policy Network Activation)\\ntanh\\nrelu\\nelu\\nFigure 12: ACKTR Policy Network Activation\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\nAverage Return\\nHalfCheetah-v1 (ACKTR, Value Network Activation)\\ntanh\\nrelu\\nelu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (ACKTR, Value Network Activation)\\ntanh\\nrelu\\nelu\\nFigure 13: ACKTR Value Network Activation\\nWe then similarly, show the signiﬁcance of these hyperparameters in the ACKTR algorithm. Our results show that the value network structure\\ncan have a signiﬁcant effect on the performance of ACKTR algorithm.\\nTrust Region Policy Optimization (TRPO)\\nFigure 14: TRPO Policy Network structure\\n',\n",
       "  'Figure 15: TRPO Value Network structure\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−750\\n−500\\n−250\\n0\\n250\\n500\\n750\\n1000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Policy Network Activation)\\ntanh\\nrelu\\nleaky relu\\nFigure 16: TRPO Policy and Value Network activation\\nFigure 17: TRPO Policy and Value Network activation\\nIn Figures 14, 15, 16, and 17 we show the effects of network structure on the OpenAI baselines implementation of TRPO. In this case, only\\nthe policy architecture seems to have a large effect on the performance of the algorithm’s ability to learn.\\n',\n",
       "  'Deep Deterministic Policy Gradient (DDPG)\\nFigure 18: Policy or Actor Network Architecture experiments for DDPG on HalfCheetah and Hopper Environment\\nWe further analyze the actor and critic network conﬁgurations for use in DDPG. As in default conﬁgurations, we ﬁrst use the ReLU activation\\nfunction for policy networks, and examine the effect of different activations and network sizes for the critic networks. Similarly, keeping critic\\nnetwork conﬁgurations under default setting, we also examine the effect of actor network activation functions and network sizes.\\n',\n",
       "  'Figure 19: Signiﬁcance of Value Function or Critic Network Activations for DDPG on HalfCheetah and Hopper Environment\\nReward Scaling Parameter in DDPG\\nFigure 20: DDPG reward rescaling on Hopper-v1, with and without layer norm.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Reward Scale, Layer Norm)\\nrs=1e-4\\nrs=1e-3\\nrs=1e-2\\nrs=1e-1\\nrs=1\\nrs=10\\nrs=100\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Reward Scale, No Layer Norm)\\nrs=1e-4\\nrs=1e-3\\nrs=1e-2\\nrs=1e-1\\nrs=1\\nrs=10\\nrs=100\\nFigure 21: DDPG reward rescaling on HalfCheetah-v1, with and without layer norm.\\nSeveral related work (Gu et al. 2016; 2017; Duan et al. 2016) have often reported that for DDPG the reward scaling parameter often needs to\\nbe ﬁne-tuned for stabilizing the performance of DDPG. It can make a signiﬁcant impact in performance of DDPG based on the choice of\\nenvironment. We examine several reward scaling parameters and demonstrate the effect this parameter can have on the stability and performance\\nof DDPG, based on the HalfCheetah and Hopper environments. Our experiment results, as demonstrated in Figure 21 and 20, show that the\\nreward scaling parameter indeed can have a signiﬁcant impact on performance. Our results show that, very small or negligible reward scaling\\nparameter can signiﬁcantly detriment the performance of DDPG across all environments. Furthermore, a scaling parameter of 10 or 1 often\\nperforms good. Based on our analysis, we suggest that every time DDPG is reported as a baseline algorithm for comparison, the reward scaling\\nparameter should be ﬁne-tuned, speciﬁc to the algorithm.\\n',\n",
       "  'Batch Size in TRPO\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\nAverage Return\\nHopper-v1 (TRPO, original, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHalfCheetah-v1 (TRPO, original, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\nFigure 22: TRPO (Schulman et al. 2015a) original code batch size experiments.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (TRPO, baselines, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−600\\n−400\\n−200\\n0\\n200\\nAverage Return\\nHalfCheetah-v1 (TRPO, baselines, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nWalker2d-v1 (TRPO, baselines, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−135\\n−130\\n−125\\n−120\\n−115\\n−110\\nAverage Return\\nReacher-v1 (TRPO, baselines, Batch Size)\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768\\nFigure 23: TRPO (Schulman et al. 2017) baselines code batch size experiments.\\nWe run batch size experiments using the original TRPO code (Schulman et al. 2015a) and the OpenAI baselines code (Schulman et al. 2017).\\nThese results can be found in Experiment results in Figure 22 and Figure 23, show that for both HalfCheetah-v1 and Hopper-v1 environments,\\na batch size of 1024 for TRPO performs best, while perform degrades consecutively as the batch size is increased.\\nRandom Seeds\\nTo determine much random seeds can affect results, we run 10 trials total on two environments using the default previously described settings\\nusign the (Gu et al. 2016) implementation of DDPG and the (Duan et al. 2016) version of TRPO. We divide our trials random into 2 partitions\\nand plot them in Figures 24 and Fig 25. As can be seen, statistically different distributions can be attained just from the random seeds with\\nthe same exact hyperparameters. As we will discuss later, bootstrapping off of the sample can give an idea for how drastic this effect will be,\\nthough too small a bootstrap will still not give concrete enough results.\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Diﬀerent Random Seeds)\\nRandom Average (5 runs)\\nRandom Average (5 runs)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\nAverage Return\\nHopper-v1 (TRPO, Diﬀerent Random Seeds)\\nRandom Average (5 runs)\\nRandom Average (5 runs)\\nFigure 24: Two different TRPO experiment runs, with same hyperparameter conﬁgurations, averaged over two splits of 5 different\\nrandom seeds.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Diﬀerent Random Seeds)\\nRandom Average (5 runs)\\nRandom Average (5 runs)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\nAverage Return\\nHopper-v1 (DDPG, Diﬀerent Random Seeds)\\nRandom Average (5 runs)\\nRandom Average (5 runs)\\nFigure 25: Two different DDPG experiment runs, with same hyperparameter conﬁgurations, averaged over two splits of 5\\ndifferent random seeds.\\nChoice of Benchmark Continuous Control Environment\\nWe previously demonstrated that the performance of policy gradient algorithms can be highly biased based on the choice of the environment.\\nIn this section, we include further results examining the impact the choice of environment can have. We show that no single algorithm can\\nperform consistenly better in all environments. This is often unlike the results we see with DQN networks in Atari domains, where results\\ncan often be demonstrated across a wide range of Atari games. Our results, for example, shows that while TRPO can perform signiﬁcantly\\nbetter than other algorithms on the Swimmer environment, it may perform quite poorly n the HalfCheetah environment, and marginally better\\non the Hopper environment compared to PPO. We demonstrate our results using the OpenAI MuJoCo Gym environments including Hopper,\\nHalfCheetah, Swimmer and Walker environments. It is notable to see the varying performance these algorithms can have even in this small set\\nof environment domains. The choice of reporting algorithm performance results can therefore often be biased based on the algorithm designer’s\\nexperience with these environments.\\n',\n",
       "  'Figure 26: Comparing Policy Gradients across various environments\\nCodebases\\nWe include a detailed analysis of performance comparison, with different network structures and activations, based on the choice of the\\nalgorithm implementation codebase.\\nFigure 27: TRPO Policy and Value Network structure\\n',\n",
       "  'Figure 28: TRPO Policy and Value Network activations.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n−250\\n0\\n250\\n500\\n750\\n1000\\n1250\\nAverage Return\\nHalfCheetah-v1 (TRPO, rllab, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\nAverage Return\\nHopper-v1 (TRPO, rllab, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n−250\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\nAverage Return\\nHalfCheetah-v1 (TRPO, rllab, Policy Network Activation)\\ntanh\\nrelu\\nleaky relu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\nAverage Return\\nHopper-v1 (TRPO, rllab, Policy Network Activation)\\ntanh\\nrelu\\nleaky relu\\nFigure 29: TRPO rllab Policy Structure and Activation\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab++, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\nAverage Return\\nHopper-v1 (DDPG, rllab++, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nAverage Return\\nHopper-v1 (DDPG, rllab++, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab++, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\nFigure 30: DDPG rllab++ Policy and Value Network structure\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab++, Policy Network Activation)\\ntanh\\nrelu\\nleaky relu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\nAverage Return\\nHopper-v1 (DDPG, rllab++, Policy Network Activation)\\ntanh\\nrelu\\nleaky\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab++, Value Network Activation)\\ntanh\\nrelu\\nleaky relu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\nAverage Return\\nHopper-v1 (DDPG, rllab++, Value Network Activation)\\ntanh\\nrelu\\nleaky relu\\nFigure 31: DDPG rllab++ Policy and Value Network activations.\\nSimilarly, Figures 32 and 33 show the same network experiments for DDPG with the Theano implementation of rllab code (Duan et al.\\n2016).\\n',\n",
       "  '0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\nAverage Return\\nHopper-v1 (DDPG, rllab, Policy Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\n2000\\nAverage Return\\nHopper-v1 (DDPG, rllab, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab, Value Network Structure)\\n(64,64)\\n(100,50,25)\\n(400,300)\\nFigure 32: DDPG rllab Policy and Value Network structure\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab, Policy Network Activation)\\ntanh\\nrelu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\nAverage Return\\nHopper-v1 (DDPG, rllab, Policy Network Activation)\\ntanh\\nrelu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\nAverage Return\\nHalfCheetah-v1 (DDPG, rllab, Value Network Activation)\\ntanh\\nrelu\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n200\\n400\\n600\\n800\\n1000\\nAverage Return\\nHopper-v1 (DDPG, rllab, Value Network Activation)\\ntanh\\nrelu\\nFigure 33: DDPG rllab Policy and Value Network activations.\\nOften in related literature, there is different baseline codebase people use for implementation of algorithms. One such example is for the\\nTRPO algorithm. It is a commonly used policy gradient method for continuous control tasks, and there exists several implementations from\\n',\n",
       "  'OpenAI Baselines (Plappert et al. 2017), OpenAI rllab (Duan et al. 2016) and the original TRPO codebase (Schulman et al. 2015a). In this\\nsection, we perform an analysis of the impact the choice of algorithm codebase can have on the performance. Figures 27 and 28 summarizes\\nour results with TRPO policy network and value networks, using the original TRPO codebase from (Schulman et al. 2015a). Figure 29 shows\\nthe results using the rllab implementation of TRPO using the same hyperparameters as our default experiments aforementioned. Note, we\\nuse a linear function approximator rather than a neural network due to the fact that the Tensorﬂow implementation of OpenAI rllab doesn’t\\nprovide anything else. We note that this is commonly used in other works (Duan et al. 2016; Stadie, Abbeel, and Sutskever 2017), but may\\ncause differences in performance. Furthermore, we leave out our value function network experiments due to this.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nAverage Return\\nHalfCheetah-v1 (DDPG, Codebase Comparison)\\nDuan 2016\\nGu 2016\\nPlapper 2017\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\nAverage Return\\nHopper-v1 (DDPG, Codebase Comparison)\\nDuan 2016\\nGu 2016\\nPlapper 2017\\nFigure 34: DDPG codebase comparison using our default set of hyperparameters (as used in other experiments).\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n−500\\n0\\n500\\n1000\\n1500\\n2000\\nAverage Return\\nHalfCheetah-v1 (TRPO, Codebase Comparison)\\nSchulman 2015\\nSchulman 2017\\nDuan 2016\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nTimesteps\\n×106\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nAverage Return\\nHopper-v1 (TRPO, Codebase Comparison)\\nSchulman 2015\\nSchulman 2017\\nDuan 2016\\nFigure 35: TRPO codebase comparison using our default set of hyperparameters (as used in other experiments).\\nFigure 35 shows a comparison of the TRPO implementations using the default hyperparamters as speciﬁed earlier in the supplemental. Note,\\nthe exception is that we use a larger batch size for rllab and original TRPO code of 20k samples per batch, as optimized in a second set of\\nexperiments. Figure 30 and 31 show the same network experiments for DDPG with the rllab++ code (Gu et al. 2016). We can then compare the\\nperformance of the algorithm across 3 codebases (keeping all hyperparameters constant at the defaults), this can be seen in Figure 34.\\nSigniﬁcance\\nOur full results from signiﬁcance testing with difference metrics can be found in Table 9 and Table 10. Our bootstrap mean and\\nconﬁdence intervals can be found in Table 13. Bootstrap power analysis can be found in Table 14. To performance signiﬁcance\\ntesting, we use our 5 sample trials to generate a bootstrap with 10k bootstraps. From this conﬁdence intervals can be obtained.\\nFor the t-test and KS-test, the average returns from the 5 trials are sorted and compared using the normal 2-sample versions of\\nthese tests. Scipy ( https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.\\nhtml, https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) and Facebook\\nBoostrapped (https://github.com/facebookincubator/bootstrapped) are used for the KS test, t-test, and bootstrap analysis.\\nFor power analysis, we attempt to determine if a sample is enough to game the signiﬁcance of a 25% lift. This is commonly used in A/B\\ntesting (Tuff´ery 2011).\\n',\n",
       "  '-\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nDDPG\\n-\\nt = 1.85, p = 0.102\\nKS = 0.60, p = 0.209\\n61.91 % (-32.27 %, 122.99 %)\\nt = 4.59, p = 0.002\\nKS = 1.00, p = 0.004\\n301.48 % (150.50 %, 431.67 %)\\nt = 2.67, p = 0.029\\nKS = 0.80, p = 0.036\\n106.91 % (-37.62 %, 185.26 %)\\nACKTR\\nt = −1.85, p = 0.102\\nKS = 0.60, p = 0.209\\n-38.24 % (-75.42 %, -15.19 %)\\n-\\nt = 2.78, p = 0.024\\nKS = 0.80, p = 0.036\\n147.96 % (30.84 %, 234.60 %)\\nt = 0.80, p = 0.448\\nKS = 0.60, p = 0.209\\n27.79 % (-67.77 %, 79.56 %)\\nTRPO\\nt = −4.59, p = 0.002\\nKS = 1.00, p = 0.004\\n-75.09 % (-86.44 %, -68.36 %)\\nt = −2.78, p = 0.024\\nKS = 0.80, p = 0.036\\n-59.67 % (-81.70 %, -46.84 %)\\n-\\nt = −2.12, p = 0.067\\nKS = 0.80, p = 0.036\\n-48.46 % (-81.23 %, -32.05 %)\\nPPO\\nt = −2.67, p = 0.029\\nKS = 0.80, p = 0.036\\n-51.67 % (-80.69 %, -31.94 %)\\nt = −0.80, p = 0.448\\nKS = 0.60, p = 0.209\\n-21.75 % (-75.99 %, 11.68 %)\\nt = 2.12, p = 0.067\\nKS = 0.80, p = 0.036\\n94.04 % (2.73 %, 169.06 %)\\n-\\nTable 9: HalfCheetah Signiﬁcance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test,\\nKolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% conﬁdence bounds.\\n-\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nDDPG\\n-\\nt = −1.41, p = 0.196\\nKS = 0.60, p = 0.209\\n-35.92 % (-85.62 %, -5.38 %)\\nt = −2.58, p = 0.033\\nKS = 0.80, p = 0.036\\n-44.96 % (-78.82 %, -20.29 %)\\nt = −2.09, p = 0.070\\nKS = 0.80, p = 0.036\\n-39.90 % (-77.12 %, -12.95 %)\\nACKTR\\nt = 1.41, p = 0.196\\nKS = 0.60, p = 0.209\\n56.05 % (-87.98 %, 123.15 %)\\n-\\nt = −1.05, p = 0.326\\nKS = 0.60, p = 0.209\\n-14.11 % (-37.17 %, 9.11 %)\\nt = −0.42, p = 0.686\\nKS = 0.40, p = 0.697\\n-6.22 % (-31.58 %, 18.98 %)\\nTRPO\\nt = 2.58, p = 0.033\\nKS = 0.80, p = 0.036\\n81.68 % (-67.76 %, 151.64 %)\\nt = 1.05, p = 0.326\\nKS = 0.60, p = 0.209\\n16.43 % (-27.92 %, 41.17 %)\\n-\\nt = 2.57, p = 0.033\\nKS = 0.60, p = 0.209\\n9.19 % (2.37 %, 15.58 %)\\nPPO\\nt = 2.09, p = 0.070\\nKS = 0.80, p = 0.036\\n66.39 % (-67.80 %, 130.16 %)\\nt = 0.42, p = 0.686\\nKS = 0.40, p = 0.697\\n6.63 % (-33.54 %, 29.59 %)\\nt = −2.57, p = 0.033\\nKS = 0.60, p = 0.209\\n-8.42 % (-14.08 %, -2.97 %)\\n-\\nTable 10: Hopper Signiﬁcance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-\\nSmirnov test, bootstrap A/B comparison % difference with 95% conﬁdence bounds.\\n-\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nDDPG\\n-\\nt = −1.03, p = 0.334\\nKS = 0.40, p = 0.697\\n-30.78 % (-91.35 %, 1.06 %)\\nt = −4.04, p = 0.004\\nKS = 1.00, p = 0.004\\n-48.52 % (-70.33 %, -28.62 %)\\nt = −3.07, p = 0.015\\nKS = 0.80, p = 0.036\\n-45.95 % (-70.85 %, -24.65 %)\\nACKTR\\nt = 1.03, p = 0.334\\nKS = 0.40, p = 0.697\\n44.47 % (-80.62 %, 111.72 %)\\n-\\nt = −1.35, p = 0.214\\nKS = 0.60, p = 0.209\\n-25.63 % (-61.28 %, 5.54 %)\\nt = −1.02, p = 0.338\\nKS = 0.60, p = 0.209\\n-21.91 % (-61.53 %, 11.02 %)\\nTRPO\\nt = 4.04, p = 0.004\\nKS = 1.00, p = 0.004\\n94.24 % (-22.59 %, 152.61 %)\\nt = 1.35, p = 0.214\\nKS = 0.60, p = 0.209\\n34.46 % (-60.47 %, 77.32 %)\\n-\\nPPO\\nt = 3.07, p = 0.015\\nKS = 0.80, p = 0.036\\n85.01 % (-31.02 %, 144.35 %)\\nt = 1.02, p = 0.338\\nKS = 0.60, p = 0.209\\n28.07 % (-65.67 %, 71.71 %)\\nt = −0.57, p = 0.582\\nKS = 0.40, p = 0.697\\n-4.75 % (-19.06 %, 10.02 %)\\n-\\nTable 11: Walker2d Signiﬁcance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-\\nSmirnov test, bootstrap A/B comparison % difference with 95% conﬁdence bounds.\\n-\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nDDPG\\n-\\nt = −2.18, p = 0.061\\nKS = 0.80, p = 0.036\\n-36.44 % (-61.04 %, -6.94 %)\\nt = −4.06, p = 0.004\\nKS = 1.00, p = 0.004\\n-85.13 % (-97.17 %, -77.95 %)\\nt = −8.33, p = 0.000\\nKS = 1.00, p = 0.004\\n-70.41 % (-80.86 %, -56.52 %)\\nACKTR\\nt = 2.18, p = 0.061\\nKS = 0.80, p = 0.036\\n57.34 % (-80.96 %, 101.11 %)\\n-\\nt = −3.69, p = 0.006\\nKS = 1.00, p = 0.004\\n-76.61 % (-90.68 %, -70.06 %)\\nt = −8.85, p = 0.000\\nKS = 1.00, p = 0.004\\n-53.45 % (-62.22 %, -47.30 %)\\nTRPO\\nt = 4.06, p = 0.004\\nKS = 1.00, p = 0.004\\n572.61 % (-73.29 %, 869.24 %)\\nt = 3.69, p = 0.006\\nKS = 1.00, p = 0.004\\n327.48 % (165.47 %, 488.66 %)\\n-\\nt = 2.39, p = 0.044\\nKS = 0.60, p = 0.209\\n99.01 % (28.44 %, 171.85 %)\\nPPO\\nt = 8.33, p = 0.000\\nKS = 1.00, p = 0.004\\n237.97 % (-59.74 %, 326.85 %)\\nt = 8.85, p = 0.000\\nKS = 1.00, p = 0.004\\n114.80 % (81.85 %, 147.33 %)\\nt = −2.39, p = 0.044\\nKS = 0.60, p = 0.209\\n-49.75 % (-78.58 %, -36.43 %)\\n-\\nTable 12: Swimmer Signiﬁcance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test,\\nKolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% conﬁdence bounds.\\n',\n",
       "  'Environment\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nHalfCheetah-v1\\n5037.26 (3664.11, 6574.01)\\n3888.85 (2288.13, 5131.96)\\n1254.55 (999.52, 1464.86)\\n3043.1 (1920.4, 4165.86)\\nHopper-v1\\n1632.13 (607.98, 2370.21)\\n2546.89 (1875.79, 3217.98)\\n2965.33 (2854.66, 3076.00)\\n2715.72 (2589.06, 2847.93)\\nWalker2d-v1\\n1582.04 (901.66, 2174.66)\\n2285.49 (1246.00, 3235.96)\\n3072.97 (2957.94, 3183.10)\\n2926.92 (2514.83, 3361.43)\\nSwimmer-v1\\n31.92 (21.68, 46.23)\\n50.22 (42.47, 55.37)\\n214.69 (141.52, 287.92)\\n107.88 (101.13, 118.56)\\nTable 13: Envs bootstrap mean and 95% conﬁdence bounds\\nEnvironment\\nDDPG\\nACKTR\\nTRPO\\nPPO\\nHalfCheetah-v1\\n100.00 %\\n0.00 %\\n0.00 %\\n79.03 %\\n11.53 %\\n9.43 %\\n79.47 %\\n20.53 %\\n0.00 %\\n61.07 %\\n10.50 %\\n28.43 %\\nHopper-v1\\n60.90 %\\n10.00 %\\n29.10 %\\n79.60 %\\n11.00 %\\n9.40 %\\n0.00 %\\n100.00 %\\n0.00 %\\n0.00 %\\n100.00 %\\n0.00 %\\nWalker2d-v1\\n89.50 %\\n0.00 %\\n10.50 %\\n60.33 %\\n9.73 %\\n29.93 %\\n0.00 %\\n100.00 %\\n0.00 %\\n59.80 %\\n31.27 %\\n8.93 %\\nSwimmer-v1\\n89.97 %\\n0.00 %\\n10.03 %\\n59.90 %\\n40.10 %\\n0.00 %\\n89.47 %\\n0.00 %\\n10.53 %\\n40.27 %\\n59.73 %\\n0.00 %\\nTable 14: Power Analysis for predicted signiﬁcance of 25% lift. Rows in cells are: % insigniﬁcant simulations,% positive\\nsigniﬁcant, % negative signiﬁcant.\\n'],\n",
       " ['Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n',\n",
       "  '1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n',\n",
       "  'Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n',\n",
       "  'Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n',\n",
       "  'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n',\n",
       "  'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\n',\n",
       "  'length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\n',\n",
       "  'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\n',\n",
       "  'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\n',\n",
       "  'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n',\n",
       "  '[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n',\n",
       "  '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\n',\n",
       "  'Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\n',\n",
       "  'The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\n',\n",
       "  'The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_text = []\n",
    "\n",
    "for fpath in glob.glob(\"../../sample_inputs/*.pdf\"):\n",
    "    with pymupdf.open(fpath) as file:\n",
    "        file_text = [page.get_text() for page in file]\n",
    "        all_files_text.append(file_text)\n",
    "\n",
    "all_files_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61fd9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6e9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing chunk 1/5 of file 1/2.\n",
      "Now processing chunk 2/5 of file 1/2.\n",
      "Now processing chunk 3/5 of file 1/2.\n",
      "Now processing chunk 4/5 of file 1/2.\n",
      "Now processing chunk 5/5 of file 1/2.\n",
      "Now processing chunk 1/3 of file 2/2.\n",
      "Now processing chunk 2/3 of file 2/2.\n",
      "Now processing chunk 3/3 of file 2/2.\n"
     ]
    }
   ],
   "source": [
    "file_summaries = []\n",
    "\n",
    "for idx, doc in enumerate(all_files_text):\n",
    "    chunked_pages = []\n",
    "    response = {\"response\": \"\"}\n",
    "\n",
    "    for i in range(math.ceil(len(doc) // 5)):\n",
    "        start_idx, end_idx = i*5, (i+1)*5\n",
    "        chunk = doc[start_idx:end_idx]\n",
    "        chunked_pages.append(\"\\n\".join([page for page in chunk]))\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(chunked_pages):\n",
    "        print(f\"Now processing chunk {chunk_idx+1}/{len(chunked_pages)} of file {idx+1}/{len(all_files_text)}.\")\n",
    "\n",
    "        per_page_summary_prompt = f\"You are an assistant that is tasked with summarizing a set of documents that are given to you. The documents will be given in chunks, and you will be given the current summary. Write a summary using the information provided. Do not reference the summary itself in your response. Do not use bullet points or any other formatting. The summary should be at least 500 words long. The text is as follows: {chunk}. The current summary is as follows: {response['response']}\"\n",
    "\n",
    "        response = generate(\n",
    "            model=\"gemma3:4b\",\n",
    "            prompt=per_page_summary_prompt,\n",
    "            options={\n",
    "                \"num_ctx\": 16384\n",
    "            }\n",
    "        )\n",
    "\n",
    "    file_summaries.append(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c8c7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep reinforcement learning has seen tremendous advancements recently, yet a persistent challenge lies in the often-disparate and inconsistent results reported across different experiments and implementations. Henderson et al.’s insightful paper, “Deep Reinforcement Learning that Matters – A Summary,” delivers a critical examination of this issue, arguing that a lack of standardized methodologies and meticulous reporting has significantly inflated performance claims within the DRL community. The core of the paper’s argument rests on the systematic analysis of benchmark environments – HalfCheetah-v1 and Hopper-v1 – commonly used in continuous control tasks, highlighting the sensitivity of algorithms like TRPO, DDPG, and PPO to subtle variations in network architecture and activation functions. \n",
      "\n",
      "The paper’s strength lies in its deliberate control over experimental parameters. Henderson et al. recognized that even seemingly minor adjustments, such as the number of layers, neurons, or activation functions (tanh, ReLU, leaky ReLU), can dramatically impact performance. They consistently report standard deviations alongside average returns, providing a more realistic assessment of algorithm reliability, moving beyond simply presenting isolated results. This statistical approach is crucial for understanding the true variance in DRL performance. The authors also effectively address the influence of codebases – particularly the OpenAI Baselines and rllab frameworks – acknowledging the potential for variation introduced by different implementations, a significant factor often overlooked.\n",
      "\n",
      "The paper’s analysis reveals that algorithms like TRPO, DDPG, and PPO possess varying degrees of complexity and sensitivity, underscoring the necessity of careful hyperparameter tuning. It effectively demonstrates the nuanced relationship between algorithm design and experimental outcomes, a point frequently glossed over in less rigorous DRL studies. The consistent presentation of standard deviations emphasizes the importance of statistical significance, moving away from a purely qualitative assessment of performance.  The authors don’t simply showcase impressive demonstrations; instead, they prioritize reproducible findings, a critical shift in the DRL landscape.\n",
      "\n",
      "However, the paper could benefit from a more explicit suggestion for establishing a standardized evaluation protocol. While Henderson et al. advocate for more rigorous reporting, they could provide more concrete guidance regarding the metrics that should be reported—perhaps including measures beyond simple average return, such as sample efficiency or robust variance—the number of trials required to achieve statistically significant results, and the appropriate statistical methods to be used.  Furthermore, a deeper exploration of potential future research directions – for example, developing more robust optimization algorithms or investigating alternative evaluation metrics—would have strengthened the paper’s overall argument and offered a clearer path forward for the field. Despite these minor suggestions, “Deep Reinforcement Learning that Matters – A Summary” provides a timely and insightful critique of the current state of DRL, emphasizing the importance of methodological rigor, transparency, and standardization – essential components for driving genuine progress in this rapidly evolving field. The paper's focus on reproducibility and statistical rigor serves as a critical reminder that impressive demonstrations alone are not sufficient for validating the true potential of deep reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "print(file_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3242fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_summaries = '\\n'.join(file_summaries)\n",
    "final_summary_prompt = f\"You are an assistant that is trying to summarize {len(file_summaries)} texts. Combine these texts into one overall summary. Do not use bullet points or any other formatting. The summary should be at least 500 words long. Here are the summaries: {all_file_summaries}\"\n",
    "\n",
    "final_summary_generation = generate(\n",
    "    model=\"gemma3:4b\",\n",
    "    prompt=final_summary_prompt,\n",
    "    options={\n",
    "        \"num_ctx\": 8192\n",
    "    }\n",
    ")\n",
    "final_summary = final_summary_generation[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cadf501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The burgeoning field of deep reinforcement learning (DRL) is currently grappling with a significant challenge: the inconsistent and often inflated reporting of performance results. Recent advancements in DRL, while impressive, are frequently hampered by a lack of standardized methodologies and meticulous reporting, a problem highlighted in Henderson et al.’s insightful paper, “Deep Reinforcement Learning that Matters – A Summary.” This paper meticulously analyzes benchmark environments – specifically HalfCheetah-v1 and Hopper-v1 – commonly used in continuous control tasks, demonstrating the remarkable sensitivity of algorithms like TRPO, DDPG, and PPO to even subtle variations in network architecture, activation functions (such as tanh, ReLU, and leaky ReLU), and experimental parameters. The authors’ strategic control over these variables, coupled with the consistent reporting of standard deviations alongside average returns, represents a critical shift toward a more realistic assessment of algorithm reliability, moving beyond simplistic, isolated demonstrations.  Crucially, the paper directly addresses the influence of codebases, particularly the OpenAI Baselines and rllab frameworks, recognizing that implementation variations can introduce significant discrepancies. \n",
      "\n",
      "The analysis reveals that algorithms like TRPO, DDPG, and PPO possess varying degrees of complexity and sensitivity, underlining the necessity of thorough hyperparameter tuning. This nuanced understanding of algorithm design and its impact on experimental outcomes is a valuable departure from less rigorous DRL studies that often gloss over these crucial details. The consistent presentation of standard deviations emphasizes the importance of statistical significance, moving away from a purely qualitative approach to performance evaluation. Henderson et al.'s work emphasizes reproducibility, a critical component often lacking in the rapidly evolving DRL landscape. However, the paper could benefit from a more explicit recommendation for establishing a standardized evaluation protocol, perhaps including metrics beyond simple average return, such as sample efficiency or robust variance, alongside detailed guidelines for determining the appropriate number of trials required for statistically significant results and the selection of suitable statistical methods. \n",
      "\n",
      "Simultaneously, a completely different but equally transformative innovation has emerged in the realm of sequence transduction, largely spearheaded by Vaswani et al. with their Transformer architecture. This architecture has revolutionized machine translation, primarily by completely replacing recurrent neural networks with a self-attention mechanism.  This fundamental shift eliminates the limitations inherent in sequential models – their struggles with long-range dependencies and their inability to efficiently utilize parallel computation. The resulting state-of-the-art translation quality, combined with dramatically reduced training times and costs compared to traditional methods, represents a significant advance.\n",
      "\n",
      "The Transformer’s core is its layered architecture comprising encoder and decoder stacks, each containing a multi-head self-attention module and a point-wise feed-forward network. The self-attention mechanism allows each input position to directly attend to all other positions, dynamically weighting their relevance—a key departure from sequential models’ sequential processing and bottlenecks.  The multi-head attention – eight heads with a dimension of 64 – further enhances this capability by enabling the model to capture diverse relationships between words. Scaling factor design was crucial for mitigating vanishing gradients and ensuring stable training, even with high-dimensional representations. The interconnected encoder and decoder layers facilitate bidirectional information flow, bolstering translation accuracy. The point-wise feed-forward networks introduce non-linear transformations, allowing the model to learn complex, position-dependent features.  The architecture’s design prioritizes maximum parallelization, allowing simultaneous computations across all input positions. \n",
      "\n",
      "Experimental results showcased the Transformer’s remarkable capabilities, achieving state-of-the-art BLEU scores on both the WMT 2014 English-to-German and English-to-French translation tasks within just 3.5 days of training on eight P100 GPUs, significantly outperforming previous ensemble methods. This versatility extends beyond translation, demonstrating success in English constituency parsing with both large and limited datasets. The authors’ in-depth analysis of attention distributions and the learned relationships between words provides valuable insights into the model’s underlying principles.  Ultimately, the Transformer represents a fundamental paradigm shift in sequence modeling, highlighting the superiority of the approach through parallelization and efficient scaling. \n",
      "\n",
      "Ultimately, both advancements – the emphasis on rigorous methodology within DRL, championed by Henderson et al., and the transformative innovation of the Transformer architecture – underscore the importance of methodological rigor, transparency, and standardization for driving genuine progress. Both highlight the need to move beyond impressive demonstrations as a sole measure of success, instead prioritizing reproducibility and statistical evidence to truly validate the potential of these rapidly evolving fields.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
